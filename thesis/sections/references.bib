@mastersthesis{DPPproject,
  author  = {Amar Topalovic, Walter Restelli-Nielsen, Kristian Olesen},
  title   = {Multiple-precision Integer Arithmetic},
  school  = {University of Copenhagen},
  year    = {2022},
  month   = {January},
  type    = {{Data Parallel Programming} Course Project},
  note    = {\url{https://futhark-lang.org/student-projects/dpp21-mpint.pdf}},
}
@misc{watt2023efficient,
      title={Efficient Generic Quotients Using Exact Arithmetic}, 
      author={Stephen M. Watt},
      year={2023},
      eprint={2304.01753},
      archivePrefix={arXiv},
      primaryClass={cs.SC}
}
@mastersthesis{CGBN,
  author  = {NVlabs},
  title   = {CGBN: CUDA Accelerated Multiple Precision Arithmetic (Big Num) using Cooperative Groups },
  school  = {GitHub},
  note    = {\url{https://github.com/NVlabs/CGBN}},
}

@misc{bassil2012sequential,
      title={Sequential and Parallel Algorithms for the Addition of Big-Integer Numbers}, 
      author={Youssef Bassil and Aziz Barbar},
      year={2012},
      eprint={1204.0232},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      note={\url{https://doi.org/10.48550/arXiv.1204.0232}}
}

@article{Maza_2010,
doi = {10.1088/1742-6596/256/1/012009},
url = {https://dx.doi.org/10.1088/1742-6596/256/1/012009},
year = {2010},
month = {nov},
publisher = {},
volume = {256},
number = {1},
pages = {012009},
author = {Marc Moreno Maza and Wei Pan},
title = {Fast polynomial multiplication on a GPU},
journal = {Journal of Physics: Conference Series},
abstract = {We present CUDA implementations of Fast Fourier Transforms over finite fields. This allows us to develop GPU support for dense univariate polynomial multiplication leading to speedup factors in the range 21 – 37 with respect to the best serial C-code available to us, for our largest input data sets. Since dense univariate polynomial multiplication is a core routine in symbolic computation, this is promising result for the integration of GPU support into computer algebra systems.}
}

@article{Emmart2010HighPI,
  title={High Precision Integer Addition, Subtraction and Multiplication with a Graphics Processing Unit},
  author={Niall Emmart and Charles C. Weems},
  journal={Parallel Process. Lett.},
  year={2010},
  volume={20},
  pages={293-306},
  url={https://api.semanticscholar.org/CorpusID:46446827}
}
@article{Bantikyan2014BigIM,
  title={Big Integer Multiplication with CUDA FFT(cuFFT) Library},
  author={Hovhannes Bantikyan},
  journal={International Journal of Innovative Research in Computer and Communication Engineering},
  year={2014},
  volume={2},
  pages={6317-6325},
  url={https://api.semanticscholar.org/CorpusID:14759606}
}

@article{doi:10.1177/10943420221077964,
author = {Adrian P Dieguez and Margarita Amor and Ramón Doallo and Akira Nukada and Satoshi Matsuoka},
title ={Efficient high-precision integer multiplication on the GPU},

journal = {The International Journal of High Performance Computing Applications},
volume = {36},
number = {3},
pages = {356-369},
year = {2022},
doi = {10.1177/10943420221077964},

URL = { 
    
        https://doi.org/10.1177/10943420221077964
    
    

},
eprint = { 
    
        https://doi.org/10.1177/10943420221077964
    
    

}
,
    abstract = { The multiplication of large integers, which has many applications in computer science, is an operation that can be expressed as a polynomial multiplication followed by a carry normalization. This work develops two approaches for efficient polynomial multiplication: one approach is based on tiling the classical convolution algorithm, but taking advantage of new CUDA architectures, a novelty approach to compute the multiplication using integers without accuracy lossless; the other one is based on the Strassen algorithm, an algorithm that multiplies large polynomials using the FFT operation, but adapting the fastest FFT libraries for current GPUs and working on the complex field. Previous studies reported that the Strassen algorithm is an effective implementation for “large enough” integers on GPUs. Additionally, most previous studies do not examine the implementation of the carry normalization, but this work describes a parallel implementation for this operation. Our results show the efficiency of our approaches for short, medium, and large sizes. }
}

@misc{marlow2010haskell,
  title={Haskell 2010 language report},
  author={Marlow, Simon and others},
  year={2010},
  howpublished={\url{https://www.haskell.org/onlinereport/haskell2010/}},
  note = {Accessed: May 9th 2024}
}

@misc{futguide,
  title = {Futhark User’s Guide},
  howpublished = {\url{https://futhark.readthedocs.io/en/latest/}},
  note = {Accessed: May 5th 2024}
}

@misc{gpuspecs,
  title = {TechPowerUp GPU Database},
  howpublished = {\url{https://www.techpowerup.com/gpu-specs/geforce-gtx-1650-super.c3411}},
  note = {Accessed: May 6th 2024}
}

@misc{GMP,
  author = {Torbjörn Granlund et al.},
  title = {GNU multiple precision arithmetic library version 6.3.0 Manual},
  howpublished = {\url{https://gmplib.org/manual/index}},
  note = {Accessed: May 9th 2024}
}

@book{knuth97,
  added-at = {2015-06-04T07:16:19.000+0200},
  address = {Boston},
  author = {Knuth, Donald E.},
  biburl = {https://www.bibsonomy.org/bibtex/25dbc415549a1bb86bff7a3842765c31f/ytyoun},
  edition = {Third},
  interhash = {b825ccd550f92a93eefbacd1bec78704},
  intrahash = {5dbc415549a1bb86bff7a3842765c31f},
  isbn = {0201896842 9780201896848},
  keywords = {algorithm knuth no.pdf taocp textbook},
  publisher = {Addison-Wesley},
  refid = {174763889},
  timestamp = {2015-07-29T09:31:05.000+0200},
  title = {The Art of Computer Programming, Volume 2: Seminumerical Algorithms},
  year = 1997
}

@misc{PMPH,
  author        = {Cosmin E. Oancea},
  title         = {Programming Massively Parallel Hardware},
  year          = {2021},
  month         = {Course taught at DIKU},
  howpublished={\url{https://github.com/diku-dk/pmph-e2021-pub/}}
}

@book{ParallelProgrammingInFuthark,
 author = {Martin Elsman and Troels Henriksen and Cosmin E. Oancea},
 title = {Parallel Programming in Futhark},
 year = {2018},
 url = "https://futhark-book.readthedocs.io"
}

@inproceedings{Henriksen:2017:FPF:3062341.3062354,
 author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
 title = {Futhark: Purely Functional GPU-programming with Nested Parallelism and In-place Array Updates},
 booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2017},
 year = {2017},
 isbn = {978-1-4503-4988-8},
 location = {Barcelona, Spain},
 pages = {556--571},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3062341.3062354},
 doi = {10.1145/3062341.3062354},
 acmid = {3062354},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, compilers, functional language, parallel},
}

@inproceedings {futhark/sc22mem,
author = {P. Munksgaard and T. Henriksen and P. Sadayappan and C. Oancea},
booktitle = {2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC) (SC)},
title = {Memory Optimizations in an Array Language},
year = {2022},
volume = {},
issn = {2167-4337},
pages = {424-438},
abstract = {We present a technique for introducing and optimizing the use of memory in a functional array language, aimed at GPU execution, that supports correct-by-construction parallelism. Using linear memory access descriptors as building blocks, we define a notion of memory in the compiler IR that enables cost-free change-of-layout transformations (e.g., slicing, transposition), whose results can even be carried across control flow such as ifs/loops without manifestation in memory. The memory notion allows a graceful transition to an unsafe IR that is automatically optimized (1) to mix reads and writes to the same array inside a parallel construct, and (2) to map semantically different arrays to the same memory block. The result is code similar to what imperative users would write. Our evaluation shows that our optimizations have significant impact (1:1â€“2) and result in performance competitive to hand-written code from challenging benchmarks, such as Rodiniaâ€™s NW, LUD, Hotspot.},
keywords = {gpu;parallelism;functional programming;optimizing compiler},
doi = {},
url = {https://doi.ieeecomputersociety.org/},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov}
}
