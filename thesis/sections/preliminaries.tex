\section{Preliminaries}
\label{sec:pre}
Parallel programming comes in two forms: Task parallelism and data
parallelism. Task parallelism means single processor, multiple memory regions,
where data parallelism means multiple processors, single memory region. In this
Thesis, we are interested in utilizing a data parallel programming environment
to speedup traditional arithmetics of big integers.

A Central Processing Unit (CPU) is not suitable for data parallel execution, as
it is specialized in task parallelism (i.e. controlling the many concurrent
tasks often running on a computer). However, many modern computers have a
Graphics Processing Unit (GPU) as well, which is a specialized hardware
component for handling the many vectors that constitutes computer graphics.
Vectorized computations are data parallel by nature, and thus, GPUs provides a
suitable architecture for this Thesis. We call it General Purpose GPU (GPGPU)
usage, since we do not intent to process graphics.

Compared to a CPU, GPGPU has the benefits of higher instruction throughput
(e.g. by increasing the core count) and higher bandwidth (e.g. by decreasing
memory management), but comes with the disadvantage of almost no dynamic control
flow or memory management abstractions. For our purpose, we can thus achieve
significant performance efficiency in exchange for more complex algorithms and
programmability.

In this section, we present GPGPU hardware architecture and parallel execution
in \ref{subsec:gpgpu}, the CUDA programming language in \ref{subsec:cuda}, and
the Futhark programming language in \ref{subsec:futhark}.

\subsection{GPGPU Architecture}
\label{subsec:gpgpu}

There are two main interfaces for interacting with a GPU: OpenCL and CUDA, the
former being an API for a open set of standards and instructions, and the latter
a proprietary API for NVIDIA GPUs consisting of specialized instructions. In
this Thesis, we focus on NVIDIA GPU architecture, as these are I) highly
efficient, II) there exists a state-of-the-art big integer library in CUDA
called CGBN \cite{CGBN}, which we can use as a base of comparison, and III) the
Futhark contributions can compile to OpenCL (and hopefully remain efficient).

GPGPU is inherently parallel, and consists of up to thousands of multithreaded
cores. The architecture, according to NVIDIA's CUDA model and language manual
\cite{cudaguide}, is as follows: Threads are grouped in 32 called
\textit{warps}, where each warp executes one instruction at a time in lockstep,
called Single Instruction, Multiple Threads (SIMT). A program, called a
\textit{kernel}, is executed on a \textit{grid} of \textit{blocks} (also
called thread blocks) with a specific amount of threads per block. The number of
threads in a block (block size) and the number of blocks in a grid (block count)
must be statically known prior to kernel execution, with maximum block size of
1024 threads.

SIMT allows writing parallel programs of minimal overhead w.r.t.\ synchronization
and memory management. Each wrap, block, and grid has its own memory called
\textit{local}, \textit{shared}, and \textit{global} memory, respectively, with
the former being the smallest and fastest, and the latter the largest and
slowest. This urges to keep computations closest to wrap-level. It also
introduce the idea of coalesced memory accesses, where consecutive threads read
consecutive memory locations, improving the amount of warp-level cache
hits. E.g.\ if it is necessary for a thread to read consecutive elements from
global memory (uncoalesced), it is generally faster to first read coalesced from
global to shared memory, followed by the original uncoalesced read from shared to
local memory. Figure \ref{fig:coalesced} visualizes this example.

\begin{figure}
  \centering
  {
    \centering
    \begin{tabular}{C{0.45\textwidth}|C{0.45\textwidth}}
      \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
        \hline
        $g_0$ & $g_1$ & $g_2$ & $g_{3}$ & $g_{4}$ & $g_{5}$ & $g_6$ & $g_7$\\
        \hline
      \end{tabular}
      &
      \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
        \hline
        $g_0$ & $g_1$ & $g_2$ & $g_{3}$ & $g_{4}$ & $g_{5}$ & $g_6$ & $g_7$\\
        \hline
      \end{tabular}\\
\begin{tabular}{C{0.003cm}C{0.003cm}C{0.42cm}C{0.003cm}C{0.003cm}C{0.42cm}C{0.003cm}C{0.003cm}C{0.4cm}C{0.003cm}C{0.003cm}C{0.12cm}}
        $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & \\
      \end{tabular}
              &
      \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
        $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$  & $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$ & $~\shortuparrow_{r}$ \\
      \end{tabular}\\[-0.7ex]
\begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
        $~t_0$ &  & $~t_1$ & & $~t_{2}$ & & $~t_3$ & \\
      \end{tabular}
              &
      \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
        $~t_0$ & $~t_1$  & $~t_2$ & $~t_3$ & $~t_{0}$ & $~t_1$ & $~t_2$ & $~t_3$ \\
      \end{tabular}\\[-0.7ex]
      &
      \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
        $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$  & $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$ & $~\shortdownarrow_{w}$ \\
      \end{tabular}\\
      &
      \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
        \hline
        $s_0$ & $s_1$ & $s_2$ & $s_{3}$ & $s_{4}$ & $s_{5}$ & $s_6$ & $s_7$\\
        \hline
      \end{tabular}\\
      &
      \begin{tabular}{C{0.003cm}C{0.003cm}C{0.42cm}C{0.003cm}C{0.003cm}C{0.42cm}C{0.003cm}C{0.003cm}C{0.4cm}C{0.003cm}C{0.003cm}C{0.12cm}}
        $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & & $\shortuparrow_{r}$ & $\diagonalarrow$ & \\
      \end{tabular}\\[-0.7ex]
      &
      \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
        $~t_0$ &  & $~t_1$ & & $~t_{2}$ & & $~t_3$ & \\
      \end{tabular}\\
      \end{tabular}
    }
    \caption{\footnotesize Coalesced memory use example. Left-hand-side shows 4
      threads reading 8 elements uncoalesced from global memory $G$, while
      right-hand-side shows a 2-step coalesced read using shared memory $S$.}
    \label{fig:coalesced}
  \end{figure}


An important side-effect of SIMT is no branch prediction and no divergent
control flow within warps. I.e.\ suppose only 1 thread in a warp choose an
\fun{if}-branch; then, the remaining 31 threads cannot proceed execution before
the first thread has finished. Instead, the 31 threads executes the
\texttt{if}-branch in lockstep with the 1 thread (on dummy data).

Thus, in order to write efficient parallel programs on a GPGPU, we must reduce
the amount of branches and make use of fast memory, optimizing throughput and
bandwidth.

\subsection{The CUDA Programming Interface}
\label{subsec:cuda}

Programming in CUDA is by means of a C++ language extension that compiles to C
using the CUDA driver API, as described in the CUDA manual \cite{cudaguide}. The
CPU running the compiled program is called the \textit{host} and the target GPU
is called the \textit{device}. The extension exports some functions in the
\textit{alloc}-family, that allows the host to preallocate, read, and write
memory to/from the device. Device functions (i.e. kernels) are called from the
host and take parameters in form of pointers to memory that has been transferred
to the device.

The extension exports some function execution- and variable memory- space
specifiers. These infer to the compiler how to treat functions and memory (e.g.\
host or device function, shared or local memory, etc.). The extension also
exports some device functions, most notably \texttt{\_\_syncthreads()} which
prevents threads from continuing execution until every thread within the block
are at that function.

To accommodate the GPU architecture, the types, data structures, and control flow
of a kernel must be statically known before run time.  In order to keep kernels
parametric, e.g. in the type or size of integers, we can use C++ templates. They
provide generic programming over type and value parameters that are concretized
at compile time, as written by Stroustrup in \cite{stroustrup}. Templates
especially prove to be a strong tool when combined with other high-level C++
constructs, such as classes. Furthermore, since the control flow is
predetermined, the device has no error handling. Instead, the host must
explicitly handle errors by peeking at the device error status, making CUDA
programming prone to bugs.

Lastly, threads are implicit: Kernels are launched with an \textit{execution
  configuration} of the form ``\fun{ker<<< grid, block >>>(params)}'', where
\fun{grid} and \fun{block} specifies the grid and block dimensions,
respectively, and are of type \fun{dim3} specifying dimensions in 3D. The kernel
function then has access to a special set of built-in variables, notably
\fun{threadIdx} and \fun{blockIdx}, containing the index of the current thread
and block, respectively. Thus, when a kernel is executed, the code of the kernel
runs on each of the specified threads, and we must use these special variables
for explicit thread indexing.

From now on, we assume:
\begin{itemize}
  \renewcommand\labelitemi{--}
\item Kernel dimensions to be 1D, so \texttt{threadIdx.x} refer to the current thread index.
  \item The existence of sensible host functions to execute and error-check kernels.
  \item A small library of common subroutines such as accessing global memory coalesced.
\end{itemize}
These assumptions allow us to focus on the implementation of algorithms.

\subsection{The Futhark Programming Language}
\label{subsec:futhark}

Futhark is a high-level pure functional language with emphasis on data-parallel
array programming, as described in its guide and book
\cite{futguide,ParallelProgrammingInFuthark}. It is hardware-agnostic and can
compile to both sequential or parallel C, OpenCL, and CUDA code. The fundamental
design of Futhark revolves around \textit{Second-Order Array Combinators}
(SOACs). They are array-functions that are easy to reason about and define in
terms sequential semantics, while still being able to compile to (efficient)
parallel code.

Analyzing the asymptotics of parallel programs is separated into \textit{work}
and \textit{span}. Work refers to the amount of computations in the program,
which we traditionally use for estimating runtimes. However, in the parallel
domain, work is distributed amongst threads, and so it may not be an accurate
representation of runtimes. The span (also called depth) is the amount of
computations by each thread given infinitely many threads.

We now present the most fundamental SOACs, which we use analyse parallelism
throughout this Thesis. For simplicity, we assume their function inputs are
$O(1)$ in this section.

First we have the combinator \fun{map} with work $O(n)$, span $O(1)$, and type
signature
\begin{equation}
\mathtt{map}:(\tau \to \tau') \to \arr{\tau} \to \arr{\tau'}
\end{equation}
It distributes a function over an array, also known from other functional
languages such as Haskell \cite{marlow2010haskell}. It is inherently parallel
with no dependencies between the data of the array.

Next we have the combinator \fun{reduce} with work $O(n)$, span $O(\log n)$, and
type signature
\begin{equation}
\mathtt{reduce}:(\tau \to \tau \to \tau) \to \tau \to \arr{\tau} \to \tau
\end{equation}
The semantics is akin to a \fun{fold} from other functional languages, but with
one big difference: The operator must be associative and have a neutral
element. By these assumptions, the array can be accumulated in $O(\log n)$
steps.

Lastly we have the combinator \fun{scan} with work $O(n)$, span $O(\log n)$, and
type signature
\begin{equation}
\mathtt{scan}:(\tau \to \tau \to \tau) \to \tau \to \arr{\tau} \to \arr{\tau}
\end{equation}
The semantics corresponds to an accumulated reduction over the input array (also
called a prefix sum), and so the associativity and neutral element restrictions
of the operator apply.

More combinators and array functions exists, but the three presented are the
most crucial to this Thesis (and arguably to Futhark). One of the strengths of
the Futhark compiler is its ability to \textit{fuse} chains of SOACs. This allow
us to write nice and clean code, straightforwardly model parallel algorithms,
and let the compiler generate more advanced and optimized GPGPU code using less
intermediate values.

While Futhark is more expressive than CUDA, it contains compromises that allows
Futhark to compile to GPGPU code, making it restricted compared to other
high-level functional languages such as Haskell. E.g.\ Futhark allows nested
arrays as data structure, but all data must be flat on a GPGPU. Hence, the
nested arrays must be regular in order for the compiler to know how to flatten
them for execution. Another restriction is that the shape of arrays must be
statically inferred to the compiler. This is primarily inferred in the
type signature of Futhark functions.

Overall, Futhark balances easy-of-use, power of abstractions, and parallel
efficiency.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
