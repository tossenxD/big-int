\section{Classical Multiplication}
\label{sec:mul}

Multiplication is the next of the basic arithmetic operators. It is more
complicated than addition, since all digits of the multiplicand have to be
multiplied by all digits of the multiplier, leading to asymptotically quadratic
runtimes. Other more work-efficient algorithms exists, such as Karatsuba or
Schönhage–Strassen using Fast Fourier Transform (FFT), both of which are
mentioned by Knuth and part of GMP \cite{knuth97,GMP}. While classical
multiplication has more work, it is expected to run faster for small integers
due to the complexities of the work-efficient algorithms, and is what GMP use
for such integers. % TODO cite FFT and Karatsuba

This section is structured as follows: In \ref{subsec:mulalg} we present a
classical quadratic multiplication algorithm that convolutes the digits of the
operands, and we discuss how to parallelize it. In \ref{subsec:mulcud} and
\ref{subsec:mulfut} we present our CUDA and Futhark implementations,
respectively, and any further optimizations related to the
implementations. Penultimately, in \ref{subsec:mulsin} we cover the special case
of multiplication by a single precision factor, and present an algorithm that
shares the behaviour and efficiency of an addition operation. Lastly, in
\ref{subsec:mulother} we give an overview of asymptotically better algorithms
with focus on Karatsuba and FFT -- where especially FFT is promising in a GPGPU
setting.

\subsection{Algorithm}
\label{subsec:mulalg}

The classical multiplication algorithm handles one digit of the multiplier at a
time. E.g.\ in decimal, $42\cdot 21$ becomes
$1 \cdot 42 \cdot 10^{0} + 2 \cdot 42 \cdot 10^{1}$. For our big integers, we have:

\begin{definition}[classical multiplication]\label{def:clasmul}
  Multiplying integer $u\in \mathbb{N}$ by $v \in \mathbb{N}$ in the positional number system, with
  base $B$ and $m$ digits, is classically decomposed to:
\begin{equation}
  \label{eq:clasmul0}
  u \cdot v = \sum_{i=0}^{m-1}u_i\left( \sum_{j=0}^{m-1}v_j\cdot B^{j} \right)B^{i}
\end{equation}
\end{definition}

The definition is illustrated in Figure \ref{fig:tiledmult}. We make two
observations:
\begin{enumerate}[label=--]
\item The multiplicand $v$ appears inside the outer sum of the definition in
  (\ref{eq:clasmul0}).
\item Only the first $m$ sums of the illustration contributes to the result in
  our setting.\footnote{The result of multiplication is up to twice the size of
    the inputs. In order to not loose precision in our setting, a memory region
    of double size should be preallocated before the multiplication function
    call.}
\end{enumerate}

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{c}
    \begin{tabular}{C{0cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.5cm}|C{0.8cm}|C{0cm}}
      \cline{2-6}
      & $w_0$ & $w_1$ & $w_2$ & $\ldots$ & $w_{m-1}$ & $\ldots$\\
      \cline{2-6}
    \end{tabular}\\
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $=$ & $=$ & $=$ &  & $=$
    \end{tabular}\\[-2ex]
    \begin{tabular}{C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0cm}C{0.7cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}}
      & & $\diagonalarrowdown$ & & $\diagonalarrowdown$ & & $\diagonalarrowdown$ & $\diagonalarrowdown$ & &
    \end{tabular}\\[-1.4ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $0$ & $c_0$ & $c_1$ & $\ldots$ & $c_{m-2}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $+$ & $+$ & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $u_0 v_0$ & $u_0 v_1$ & $u_0 v_2$ & $\ldots$ & $u_0v_{m-1}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & $+$ & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & $u_1v_0$ & $u_1v_1$ & $\ldots$ & $u_1v_{m-2}$ & $\ldots$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & & $u_2v_0$ & $\ldots$ & $u_2v_{m-3}$ & $\ldots$
    \end{tabular}\\[-0.6ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & & & $\ddots$ & $\vdots$
    \end{tabular}\\[-0.6ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & & & & $u_{m-1}v_0$ & $\ldots$
    \end{tabular}
  \end{tabular}
  \caption{\footnotesize Visualization of classical multiplication as stated in Definition \ref{def:clasmul} with the outer sum as rows.}
  \label{fig:tiledmult}
\end{figure}

The former tells us that we cannot compute the outer sum using register
arithmetics. However, from the latter, we can redefine the terms of the outer
sum by exploiting the tiling of digits that is apparent in the illustration. We
refer to the columns as \textit{convolutions}, e.g.\ the convolution of $w_1$ is
the sum $c_0+u_0v_1+u_1v_0$, and the arithmetics in a convolution fits in
registers. Hence, we arrive at the following definition:

\begin{definition}[tiling of classical multiplication by convolutions preserving
  input shape]\label{def:clasmultil}
  Assuming the product of Definition \ref{def:clasmul} is truncated to $m$
  digits, it can be redefined to:
\begin{equation}
\label{eq:clasmul}
u \cdot v = \sum_{k=0}^{m-1} \left( \sum_{\substack{0\leq i,j < m\\i+j=k}}u_i\cdot v_j \right)B^{k}
\end{equation}
\end{definition}

A sequential algorithm is straightforward to construct from this definition. In
contrast, we make three new observations to parallelize it:
\begin{enumerate}[label=--]
\item The products across all convolution are unique, i.e.\ the $O(m^2)$
  products of the inner sum in Equation (\ref{eq:clasmul}) have to be computed.
\item The number of terms constituting a convolution (i.e.\ the work) is linear
  in $k$.
\item The overflows can be added as a separate step after the convolutions
  (e.g.\ using \textit{badd}).
\end{enumerate}

For simplicity, let us assume we have $m$ threads available.\footnote{Since we
  parallelize at block-level, we have $m/q \leq 1024$ threads, where $q$ is the
  sequentialization factor.} The first observation tells us that each thread
must compute $O(m)$ inner products with no parallelism. It would seem natural to
let thread $t_{l\in\{0,..,m-1\}}$ compute convolution $l$, but from the second
observation, this gives an unbalanced amount of sequential work amongst
threads. However, we also derive from the second observation that the combined
work of convolution $0$ and convolution ${m-1}$ is $m+1$, and is equal to that
of convolution $1$ and ${m-2}$, and to that of $2$ and ${m-3}$,
etc.\footnote{Easy to see on the pattern visualized in Figure
  \ref{fig:tiledmult}} Thus, by introducing a fixed sequentialization factor of
$2$, we can balance the work amongst threads, s.t.\ thread
$t_{l\in 0,..,(m/2)-1}$ computes convolution $l$ and convolution $m-1-l$.

Regarding the third observation; in order to propagate the overflows in a
separate sweep, we must keep track of them while computing the convolutions. It
is a known rule that; given any two factors, their product fit in their combined
size. E.g.\ in decimal, $9$ (size 1) times $99$ (size 2) is $891$ (size 3). By
this rule, each product of a convolution fits in two machine words. We denote
the least significant word as the \textit{low} part and the most significant as
the \textit{high} part. Now, each convolution is the sum of $O(m)$ low and high
parts. Adding the low parts overflows to the high part, and adding the high part
overflows to the \textit{carry} part -- yet another machine word (assuming the
base is big enough to hold the carry part). Hence, we need three words to keep
track of each convolution.

Thus, we derive the following parallel algorithm with work $O(m^2)$ and span
$O(m)$:

\begin{definition}[work balanced parallel algorithm for classical
  \textit{mul}tiplication by \textit{conv}olution
  (\textit{convmul})]\label{def:convmul}
  We parallelize Definition \ref{def:clasmul} according to the pseudocode in
  Figure \ref{fig:mulparalg}. Lines 1-18 describes the main function that, given
  a thread index ${t\in \{0,..,(m/2)-1\}}$, computes a low, high, and carry part
  of both convolution $t$ and convolution $m-1-t$. Lines 20-25 calls the main
  function on each thread and adds the convolution parts according to following
  memory layout, where color and superscript denotes what thread has computed
  the part and subscript denotes what convolution the part comes from:
\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.5cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|}
      \hline
      \red $l^0_0$ & \blue $l^1_1$ & \green $l^2_2$ & \brown $l^3_3$ & $\ldots$ & \brown $l^3_{m-4}$ & \green $l^2_{m-3}$ & \blue $l^1_{m-2}$ & \red $l^0_{m-1}$\\
      \hline
    \end{tabular}\\[-0.3ex]
    \begin{tabular}{C{0.81cm}C{0.81cm}C{0.81cm}C{0.81cm}C{0.51cm}C{0.81cm}C{0.81cm}C{0.81cm}C{0.81cm}}
      $+$ & $+$ & $+$ & $+$ &  & $+$ & $+$ & $+$ & $+$
    \end{tabular}\\[-0.3ex]
    \begin{tabular}{|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.5cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|}
      \hline
      $0$ & \red $h^0_0$ & \blue $h^1_1$ & \green $h^2_2$ & $\ldots$ & \color{magenta} $h^{4}_{m-5}$ & \brown $h^{3}_{m-4}$ & \green $h^{2}_{m-3}$ & \blue $h^1_{m-2}$\\
      \hline
    \end{tabular}\\[-0.3ex]
    \begin{tabular}{C{0.81cm}C{0.81cm}C{0.81cm}C{0.81cm}C{0.51cm}C{0.81cm}C{0.81cm}C{0.81cm}C{0.81cm}}
      $+$ & $+$ & $+$ & $+$ &  & $+$ & $+$ & $+$ & $+$
    \end{tabular}\\[-0.3ex]
    \begin{tabular}{|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.5cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|}
      \hline
      $0$ & $0$ & \red $c^{0}_0$ & \blue $c^{1}_1$ & $\ldots$ & \color{cyan} $c^{5}_{m-6}$ & \color{magenta} $c^{4}_{m-5}$ & \brown $c^{3}_{m-4}$ & \green $c^{2}_{m-3}$\\
      \hline
    \end{tabular}
  \end{tabular}
\end{center}
\end{definition}~

\begin{figure}[H]
\begin{lstlisting}[language=pseudo,escapeinside={(*}{*)},frame=single]
fun CONV t = -- Parameter `t` represents current index of the m/2 threads
    k1 = t
    k2 = m - 1 - k1            -- The indices `k1` and `k2` represents the
    l1, l2, h1, h2, c1, c2 = 0 -- upper and lower `k` handled by thread `t`

    for i in (0..k1)        -- The indices `i` and `j` are computed
        j = k1 - i          -- straightforward w.r.t. Equation ((*{\hypersetup{allcolors=ForestGreen}\ref{eq:clasmul}}*))
        l1 += u[i] *(*$_{\mathtt{low}}$*) v[j]
        h1 += u[i] *(*$_{\mathtt{high}}$*) v[j] + overflow(*$_{\mathtt{l1}}$*)
        c1 += overflow(*$_{\mathtt{h1}}$*)

    for i in (0..k2)
        j = k2 - i
        l2 += u[i] *(*$_{\mathtt{low}}$*) v[j]
        h2 += u[i] *(*$_{\mathtt{high}}$*) v[j] + overflow(*$_{\mathtt{l2}}$*)
        c2 += overflow(*$_{\mathtt{h2}}$*)

    return (l1, l2, h1, h2, c1, c2)

(l1, l2, h1, h2, c1, c2) = map CONV (0..(m/2)-1)
l = concat l1 (reverse l2)           -- The second half of the
h = shift 1 (concat h1 (reverse h2)) -- convolutions are computed in
c = shift 2 (concat c1 (reverse c2)) -- reverse order due to work balancing
r = ADD l h
w = ADD r c
\end{lstlisting}
  \caption{\footnotesize Pseudocode of work balanced parallel algorithm for classical multiplication by convolution of big integers. \texttt{Input:} $u$ and $v$ of size $m$. \texttt{Output:} $w$ of size $m$. \texttt{Use:} Big integer addition function \texttt{ADD}.}
  \label{fig:mulparalg}
\end{figure}

While we cannot optimize the asymptotics of the algorithm, we can eliminate one
of the additions, decreasing the amount of communication. The intuition is to
compute two lower and two upper convolutions per thread -- four in total. This
allows to prematurely compute half of the additions before the parts leave
register memory (i.e.\ in the function \texttt{CONV} of Figure
\ref{fig:mulparalg}), and thus, necessitating only one addition after running
the convolutions. The optimization not only reduces the communication, but also
reduces shared memory usage by better utilizing registers. The downside is that
it further increase the amount of sequential work within a thread by a factor of
2. Hence, whether the optimization will pay off depends on the batch size and
number of available threads -- or if the inputs are too big to fit a CUDA block
with sequentialization factor of 2.

Figure \ref{fig:muloptmem} contains an illustration of the tiled convolution
layout of this optimization w.r.t.\ threads. Now, compared to the pseudocode of
Figure \ref{fig:mulparalg}, a thread computes a total of twelve parts in the
convolution function \texttt{conv}, but combines them to eight parts before
returning -- four for the lower convolution and four for the upper. In turn, the
memory layout is trickier to construct, requiring threads to alternate writing
their parts in pairs to the additive arrays. E.g.\ with four threads we get that
the final parallel addition of the convolution-results corresponds to the
following memory layout, where color and superscript denotes what thread has
computed the part and subscript denotes the part number:

\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
      \hline
      \color{Crimson}$t^0_{00}$ & \color{Crimson}$t^0_{01}$ & \color{RoyalBlue}$t^1_{00}$ & \color{RoyalBlue}$t^1_{01}$  & \color{ForestGreen}$t^2_{00}$ & \color{ForestGreen}$t^2_{01}$  & \color{Chocolate}$t^3_{00}$ & \color{Chocolate}$t^3_{01}$  & \color{Chocolate}$t^3_{10}$ & \color{Chocolate}$t^3_{11}$  & \color{ForestGreen}$t^2_{10}$ & \color{ForestGreen}$t^2_{11}$  & \color{RoyalBlue}$t^1_{10}$ & \color{RoyalBlue}$t^1_{11}$ & \color{Crimson}$t^0_{10}$ & \color{Crimson}$t^0_{11}$ \\
      \hline
    \end{tabular}\\
    \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
      $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$
    \end{tabular}\\
    \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
      \hline
      0 & 0 & \color{Crimson}$t^0_{02}$ & \color{Crimson}$t^0_{03}$  & \color{RoyalBlue}$t^1_{02}$ & \color{RoyalBlue}$t^1_{03}$ & \color{ForestGreen}$t^2_{02}$ & \color{ForestGreen}$t^2_{03}$ & \color{Chocolate}$t^3_{02}$ & \color{Chocolate}$t^3_{03}$ & \color{Chocolate}$t^3_{12}$ & \color{Chocolate}$t^3_{13}$ & \color{ForestGreen}$t^2_{12}$ & \color{ForestGreen}$t^2_{13}$ & \color{RoyalBlue}$t^1_{12}$ & \color{RoyalBlue}$t^1_{13}$ \\
      \hline
    \end{tabular}
  \end{tabular}
\end{center}~

The optimized memory layout perfectly partitions into a parallel addition
operator with sequentialization factor of 4, and thus, yielding both the
aforementioned upsides and a faster addition.\footnote{Experimentally, we found
  addition with sequentialization factor of 4 to be the fastest (see section
  \ref{sec:add}).}  With sequentialization factor of $q=4$, it can also process
integers at CUDA block-level of up to double the size than the one with
$q=2$. To go beyond $q=4$, it is wiser to stack this optimizations rather than
combining all the consecutive sequential parts, as more combinations reduce the
effectiveness of the final parallel addition.



\begin{figure}
  \begin{center}
  \small
  \begin{tabular}{C{0.5cm}}
  \Large{\color{Crimson} $t_0$}~)\\\\
    \Large{\color{RoyalBlue} $t_1$}~)
\end{tabular}
  \begin{tabular}{cccccc}
    \color{Crimson}$l_0$ & \color{Crimson}$h_0$ & \color{Crimson}$c_0$ & & & \\
    & \color{Crimson}$l_1$ & \color{Crimson}$h_1$ & \color{Crimson}$c_1$ & & \\
    & & \color{RoyalBlue}$l_2$ & \color{RoyalBlue}$h_2$ & \color{RoyalBlue}$c_2$ & \\
    & & & \color{RoyalBlue}$l_3$ & \color{RoyalBlue}$h_3$ & \color{RoyalBlue}$c_3$
  \end{tabular}
$\xrightarrow{\text{combines to}}$
\begin{tabular}{cccccc}
     & $\xrightarrow{\scriptsize \text{\color{Crimson}carry}}$ & $\xrightarrow{\scriptsize \text{\color{Crimson}carry}}$ &  &  &  \\
    \color{Crimson}$l_0$ & \color{Crimson}$h_0 + l_1$ & \color{Crimson}$c_0 + h_1$ & \color{Crimson}c$_1$ & & \\
    & & \color{RoyalBlue}$l_2$ & \color{RoyalBlue}$h_2+l_3$ & \color{RoyalBlue}$c_2+h_3$ & \color{RoyalBlue} $c_3$\\
  & & & $\xrightarrow[\scriptsize \text{\color{RoyalBlue}carry}]{}$ & $\xrightarrow[\scriptsize \text{\color{RoyalBlue}carry}]{}$ & \\
\end{tabular}
\end{center}
~~~~$\vdots$\qquad\qquad\qquad\qquad\qquad $\ddots$ \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad  $\ddots$
\caption{\footnotesize Illustration of optimized memory layout of
  \textit{convmul} with sequentialization factor of 4, where the left-hand-side
  shows the low, high, and carry parts of the first four convolutions, and the
  right-hand-side shows how to combine six parts to four on a thread-level.}
  \label{fig:muloptmem}
\end{figure}

\subsection{CUDA Implementation}
\label{subsec:mulcud}

In this section, we introduce five CUDA versions. The first version
(\texttt{V1}) implement the \textit{convmul} algorithm straightforward, with a
sequentialization factor of 2. The second version (\texttt{V2}) introduces a
data type optimization w.r.t.\ doubling the word-size for keeping track of
convolutions inside loops. This optimization effectively minimizes the amount of
work of loops, by delaying the computations of overflows until after the loop
has run.

The third version (\texttt{V3}) introduces multiple instances per block to the
approach of \texttt{V2}. The fourth version (\texttt{V4}) implements a
sequentialization factor of 4 according to the optimization that combines
consecutive convolutions, and the fifth version (\texttt{V5}) expands
\texttt{V4} with multiple instances per block.

We decide the CUDA kernel dimensions and parameters with the code in Listing
\ref{mulparams}. Akin to Listing \ref{addparams} regarding addition, but with
the block size rounded up to $128$ rather than $256$ (which we experimentally
found to be faster). The sequentialization factor does not grow beyond 4, but,
as mentioned at the end of section \ref{subsec:mulalg}, one could implement a
version that use some multiple of 4 as sequentialization factor. Hence the CUDA
implementations are limited to integers of size $m\leq4096$.

\begin{lstlisting}[language=CPP,caption={\footnotesize CUDA multiplication parameters and dimensions for version $v$ with size $m$ and $num\_instances$.},label={mulparams}]
const uint32_t q = (v >= 1 && v <= 3) ? 2 : 4;
const uint32_t ipb = (v == 3 || v == 5) ? (128 + (m/q) - 1) / (m/q) : 1;
dim3 block(ipb*(m/q), 1, 1);
dim3 grid (num_instances/ipb, 1, 1);
\end{lstlisting}

This section is structured into three paragraphs that describes the
implementation of convolutions, memory layouts, and multiple instances per
block, respectively. Combined, they establish how the five CUDA kernels are
implemented.

\paragraph{Convolutions.}
The implementations use the type \texttt{ubig\_t} of the generic base class -- a
type that use twice as many bits as the base \texttt{uint\_t}. We use the type
to compute the inner products of the convolutions, but it also allows the
optimization of \texttt{V2}: The idea is to postpone the combining of the low
and high parts of products until after the sequential loop. Then, the carries
can be propagated once instead of $i$ times for convolution $k_i$. The first two
convolutions of Listing \ref{cudaconvs} shows the difference of storing
intermediate values of the convolution in type \texttt{uint\_t} (by a low, high,
and carry part) and in type \texttt{ubig\_t} (by a low and high part only).

The third convolution of Listing \ref{cudaconvs} shows how to compute and
combine two consecutive convolutions, used in the optimized version with
sequentialization factor of 4. The CUDA listing also reveals another upside of
this optimization; by combining the two consecutive convolutions into one
loop-construct, we can reuse a digit of the multiplier $u$, and thus, reduce
memory usage further.

The three presented convolutions are used to define \texttt{V1}, \texttt{V2},
and \texttt{V4}, respectively.

\begin{lstlisting}[language=CPP,escapeinside={(*}{*)},caption={\footnotesize The three convolutions used in the CUDA multiplication kernels of file \texttt{ker-mul.cu.h}.},label={cudaconvs}]
(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k as described in Definition {\hypersetup{allcolors=Crimson}\ref{def:convmul}}}*)
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        uint_t l = (uint_t) uv;
        uint_t h = (uint_t) (uv >> Base::bits);
        // update lows, highs, and carries
        ls += l;
        hs += h + (ls < l);
        cs += hs < (h + (ls < l));
    }

(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k optimized for data type ubig\_t}}*)
    ubig_t l = 0;
    ubig_t h = 0;
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        l += uv & ((ubig_t) Base::HIGHEST);
        h += uv >> Base::bits;
    }
    // update lows, highs, and carries
    ls = (uint_t) l;
    hs = ((uint_t) h) + ((uint_t) (l >> Base::bits));
    cs = ((uint_t) (h >> Base::bits)) + (hs < ((uint_t) h));

(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k and k+1 optimized for ubig\_t and sequentialization factor of 4}}*)
    ubig_t l1 = 0; ubig_t h1 = 0;
    ubig_t l2 = 0; ubig_t h2 = 0;
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv1 = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        ubig_t uv2 = ((ubig_t) u[i]) * ((ubig_t) v[j+1]);
        l1 += uv1 & ((ubig_t) Base::HIGHEST);
        h1 += uv1 >> Base::bits;
        l2 += uv2 & ((ubig_t) Base::HIGHEST);
        h2 += uv2 >> Base::bits;
    }
    // remaining computation where i = k+1
    ubig_t uv = ((ubig_t) u[k+1]) * ((ubig_t) v[0]);
    l2 += uv & ((ubig_t) Base::HIGHEST);
    h2 += uv >> Base::bits;
    // update lows, highs, and carries
    ls1 = (uint_t) l1;
    hs1 = ((uint_t) h1) + ((uint_t) (l1 >> Base::bits));
    cs1 = ((uint_t) (h1 >> Base::bits)) + (hs1 < ((uint_t) h1));
    ls2 = (uint_t) l2;
    hs2 = ((uint_t) h2) + ((uint_t) (l2 >> Base::bits));
    cs2 = ((uint_t) (h2 >> Base::bits)) + (hs2 < ((uint_t) h2));
    // combine the lows, highs and carries
    ls = ls1;
    hsls = hs1 + ls2;
    cshs = cs1 + hs2 + (hsls < hs1);
    cscs = cs2 + (cshs < hs2);
\end{lstlisting}

\paragraph{Memory layout.}
Constructing the memory layout of \textit{convmul} with sequentialization factor
of $q=2$ (Definition \ref{def:convmul}) is straightforward; the shared memory is
partitioned in three, and each thread write their two convolution parts to each
of the three memory partitions. The writes are coalesced by design. E.g. for
eight digits and four threads, where color and superscript denotes thread and
subscript denotes convolution, the writing becomes:
\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}?C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}?C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|}
      \hline
      \color{Crimson}$l^0_{0}$ & \color{RoyalBlue}$l^1_{0}$ & \color{ForestGreen}$l^2_{0}$ & \color{Chocolate}$l^3_{0}$ & \color{Chocolate}$l^3_{1}$ & \color{ForestGreen}$l^2_{1}$ & \color{RoyalBlue}$l^1_{1}$ & \color{Crimson}$l^0_{1}$ & \color{Crimson}$h^0_{0}$ & \color{RoyalBlue}$h^1_{0}$ & \color{ForestGreen}$h^2_{0}$ & \color{Chocolate}$h^3_{0}$ & \color{Chocolate}$h^3_{1}$ & \color{ForestGreen}$h^2_{1}$ & \color{RoyalBlue}$h^1_{1}$ & \color{Crimson}$h^0_{1}$ & \color{Crimson}$c^0_{0}$ & \color{RoyalBlue}$c^1_{0}$ & \color{ForestGreen}$c^2_{0}$ & \color{Chocolate}$c^3_{0}$ & \color{Chocolate}$c^3_{1}$ & \color{ForestGreen}$c^2_{1}$ & \color{RoyalBlue}$c^1_{1}$ & \color{Crimson}$c^0_{1}$ \\
      \hline
    \end{tabular}\\[-0.4ex]
    \begin{tabular}{C{4.45cm}C{4.45cm}C{4.45cm}}
      \upbracefill & \upbracefill & \upbracefill\\[-0.3ex]
      $\mathit{ls}$ & $\mathit{hs}$ & $\mathit{cs}$
    \end{tabular}
  \end{tabular}
\end{center}
Now, reading and adding the three sub-arrays in memory is trivial using
\textit{badd} with $q=2$.

On the contrary, we get a complicated memory layout (in order to read and write
coalesced) when optimizing \textit{convmul} for $q=4$. Since we compute four
parts per convolution, we partition the shared memory in four and write the
convolution results to the partitions, coalesced. Reading from the first two
partitions is straightforward, but reading from the last two partitions is
offset by $-1$ s.t.\ if the index is out-of-bound of the partition (i.e.\ the
first thread is reading), the constant $0$ is read instead.

E.g.\ for eight digits and two threads, we get a shared memory buffer of size
$16$ and partition it to the four buffers $\mathit{ls}$, $\mathit{hls}$,
$\mathit{chs}$, and $\mathit{ccs}$. Now, in order to add the convolution parts
and access memory coalesced, the reading and writing of shared memory follows
this pattern, where color and superscript denotes thread and subscript denotes
convolution (register):
\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}}
      \color{Crimson}$\,t^0_{00}$ & \blue $\,t^1_{00}$ & \color{RoyalBlue}$\,t^1_{10}$ & \red $\,t^0_{10}$ & \color{Crimson}$\,t^0_{01}$ & \blue $\,t^1_{01}$ & \color{RoyalBlue}$\,t^1_{11}$ & \red$\,t^0_{11}$ & \color{Crimson}$\,t^0_{02}$ & \color{RoyalBlue}$\,t^1_{02}$ & \color{RoyalBlue}$\,t^1_{12}$ & \red $\,t^0_{12}$ & \color{Crimson}$\,t^0_{03}$ & \color{RoyalBlue}$\,t^1_{03}$ & \color{RoyalBlue}$\,t^1_{13}$ & \red $\,t^0_{13}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}}
      $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ & $\,~\shortdownarrow_w$ &$\,~\shortdownarrow_w$
    \end{tabular}\\
    \begin{tabular}{|C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}|}
      \hline
      $l_{0}$ & $l_{2}$ & $l_{3}$ & $l_{4}$ & $\mathit{lh}_{0}$ & $\mathit{lh}_{1}$ & $\mathit{lh}_{2}$ & $\mathit{lh}_{3}$ & $\mathit{hc}_{0}$ & $\mathit{hc}_{1}$ & $\mathit{hc}_{2}$ & $\mathit{hc}_{3}$ & $\mathit{cc}_{0}$ & $\mathit{cc}_{1}$ & $\mathit{cc}_{2}$ & $\mathit{cc}_{3}$ \\
      \hline
    \end{tabular}\\
    \begin{tabular}{C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.5cm}C{0.46cm}C{0.46cm}C{0.46cm}}
      $\,~\shortuparrow_r$ & $\,~\shortuparrow_{r}$ & $\,~\shortuparrow_{r}$ & $\,~\shortuparrow_{r}$ & $\,~\shortuparrow_{r}$ & $\,~\shortuparrow_{r}$ & $\,~\shortuparrow_{r}$  & $\,~\shortuparrow_{r}$ & \scriptsize $0_{r}$ & $\diagonalarrowleftup_{r}$ & $\diagonalarrowleftup_{r}$  & $\diagonalarrowleftup_{r}$  & \scriptsize $0_{r}$ & $\diagonalarrowleftup_{r}$  & $\diagonalarrowleftup_{r}$ & $\diagonalarrowleftup_{r}$
    \end{tabular}\\
    \begin{tabular}{C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}}
      \color{Crimson}$t^0_{00}$ & \color{Crimson}$t^0_{12}$ & \color{RoyalBlue}$t^1_{00}$ & \color{RoyalBlue}$t^1_{12}$ & \color{Crimson}$t^0_{01}$ & \color{Crimson}$t^0_{13}$ & \color{RoyalBlue}$t^1_{01}$ & \color{RoyalBlue}$t^1_{13}$ & \color{Crimson}$t^0_{10}$ & \color{Crimson}$t^0_{02}$ & \color{RoyalBlue}$t^1_{10}$ & \color{RoyalBlue}$t^1_{02}$ & \color{Crimson}$t^0_{11}$ & \color{Crimson}$t^0_{03}$ & \color{RoyalBlue}$t^1_{11}$ & \color{RoyalBlue}$t^1_{03}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{3.2cm}C{3.2cm}C{3.2cm}C{3.2cm}}
      \upbracefill & \upbracefill & \upbracefill & \upbracefill\\[-0.3ex]
      $\mathit{ls}$ & $\mathit{hls}$ & $\mathit{chs}$ & $\mathit{ccs}$
    \end{tabular}
  \end{tabular}
\end{center}
Afterwards, the registers in threads contains four pairs of elements to be
added, and thus, can straightforward run \textit{badd} with sequentialization
factor of 4 and pre-fetched values to registers -- corresponding to the addition
of this optimization at the end of section \ref{subsec:mulalg}. Listing
\ref{cudamulmem} in Appendix \hyperref[app:B]{B} contains the CUDA code for
reading and writing in this pattern.

The two memory layouts are used to define define \texttt{V1} \& \texttt{V2} and
\texttt{V4}, respectively.

\paragraph{Multiple instances per block.}
When handling multiple arithmetic instances in a CUDA block at the same time,
the symmetrical pattern remains w.r.t.\ the work of convolutions. E.g.\ 4
instances of size 4 have the following convolution pattern, where color denotes
instance:
\begin{center}
  \begin{tabular}{r}
    \begin{tabular}{|C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \cgray & \cgray &\cgray  &\cgray \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cgray & \cgray & \cgray\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cgray & \cgray\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  & \cgray \\
      \cline{4-4}
    \end{tabular}
  \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
      \cbeige &\cbeige  &\cbeige  &\cbeige \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cbeige & \cbeige & \cbeige\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cbeige &\cbeige \\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  &\cbeige \\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \csteelblue& \csteelblue &\csteelblue  &\csteelblue \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \csteelblue & \csteelblue & \csteelblue\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \csteelblue & \csteelblue\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  & \csteelblue\\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \cpurple& \cpurple &\cpurple  &\cpurple \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cpurple & \cpurple & \cpurple\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cpurple & \cpurple\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  &\cpurple \\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
\end{center}
Hence, the work-balancing pattern still applies (i.e.\ indexing from both ends
in a thread).

Thus, we use the same convolution pattern to define \texttt{V3} and \texttt{V5}
as we use to define \texttt{V2} and \texttt{V4}, respectively. In turn, the
memory layouts from \texttt{V2} and \texttt{V4} applies to \texttt{V3} and
\texttt{V5} as well. However, we have to be careful when reading from the memory
buffer of the second additive array, as shifting the layout is no longer
sufficient to get the correct tiling. Instead, we compute the thread-index
w.r.t.\ segments s.t.\ the first thread of a segment fetch $0$ rather than
indexing into to the previous segment. The aforementioned CUDA code for the
memory layout of \texttt{V4} (Appendix \hyperref[app:B]{B} Listing
\ref{cudamulmem}) includes this segment-beginning check.

\subsection{Futhark Implementation}
\label{subsec:mulfut}
{\red [TODO; mostly missing]}

The implementation of \textit{convmul} in Futhark has one significant change
from CUDA: There may not be a double-size type for the base type, and so we
cannot always apply the optimization of postponing the carry propagation. In
Futharks high-level design, we do not get all the low-level C primitives, and
that means no efficient 128-bit words.

The first version (\texttt{V1}) implements the algorithm with sequentialization
factor of 2, the second (\texttt{V2}) with a factor of 4, and the third
(\texttt{V3}) allows segmented multiplication.

\subsection{Single Precision Factor}
\label{subsec:mulsin}

Integer multiplication has some special cases; we have multiplication by 0, by 1
and by the radix. Respectively, this is equivalent to 0, identity and
right-shift (e.g.\ $4 \cdot 10^1$ in decimal is equivalent to $4 \ll 1$, and
$4\cdot 10^2$ to $4\ll 2$, etc.). Multiplication with big integers has another special
case; multiplication by a single precision factor.

This case can be computed in parallel in three steps, using parallel addition:
\begin{enumerate}[label=\Roman*]
\item Multiply each digit of the big integer by the single precision factor,
  resulting in two arrays -- one with the low parts and one with the high parts
  of the multiplication.
\item Shift the array with the high parts by 1.
\item Add the two arrays using parallel addition (e.g.\ \textit{badd}).
\end{enumerate}
Figure \ref{fig:muld} contains a illustration and the pseudocode of this
algorithm.

\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
    \small
    \texttt{Input:} $u$ of size $m$ base $B$ and digit $d$\\
    \texttt{Output:} $w$ of size $m$ in base $B$\\
    \texttt{Use:} Function \texttt{ADD} for adding big ints
\begin{lstlisting}[language=pseudo,frame=,escapeinside={(*}{*)}]
l = map (*(*$_{\mathtt{low}}$*) d) u
h = map (*(*$_{\mathtt{high}}$*) d) u
h = shift 1 h
w = ADD l h
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
    \centering
    \footnotesize
    \begin{tabular}{c}
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $u_0$ & $u_1$ & $u_2$ & $\cdots$ & $u_{m-1}$\\ 
        \hline
      \end{tabular}\\[-0.3ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}}
        $\cdot$ & $\cdot$ & $\cdot$ & & $\cdot$\\ 
      \end{tabular}\\[-0.7ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}}
        $d$ & $d$ &  $d$ & $\cdots$ &$d$ \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.8cm}C{0.7cm}C{0.7cm}}
        $=$ & $=$ & $=$ &  & $=$  
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $h_{0}$ & $h_1$ & $h_2$ & $\cdots$ & $h_{m-1}$\\
        \hline
      \end{tabular}\\
      \begin{tabular}{C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}}
        \diagonalarrowdown{}$_+$ & & \diagonalarrowdown{}$_+$ &  & \diagonalarrowdown{}$_+$  &  & \diagonalarrowdown{}$_+$ \\
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $l_{0}$ & $l_1$ & $l_2$ & $\cdots$ & $l_{m-1}$\\
        \hline
      \end{tabular}\\
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.8cm}C{0.7cm}C{0.7cm}}
        $=$ & $=$ & $=$ &  & $=$  
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $w_{0}$ & $w_1$ & $w_2$ & $\cdots$ & $w_{m-1}$\\
        \hline
      \end{tabular}
    \end{tabular}
  \end{minipage}
  \caption{\footnotesize Pseudocode and illustration of parallel algorithm for multiplication by single precision factor.}
  \label{fig:muld}
\end{figure}

We could integrate this special case (or any of the other special cases) into
the classical multiple precision multiplication implementation. However such an
integration raises two concerns: First, we expect multiple precision factors to
have the dominant occurrence over single precision (because we are in the domain
of big integers), and therefore it is not worth the cost of the extra
computational steps required to check for special cases. Second, it introduces
branching into the multiplication, which is especially inefficient for batch
processing with multiple instances per block (as diverging branches within a
block results gives unbalanced work amongst threads).

Moreover, it is essentially a parallel addition, so we do not benchmark or
discuss it further.

\subsection{Other Algorithms}
\label{subsec:mulother}

{\red [Missing -- write about FFT and mention Karatsuba.]}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
