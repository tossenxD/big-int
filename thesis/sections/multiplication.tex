\section{Classical Multiplication}
\label{sec:mul}

Multiplication is the next of the basic arithmetic operators. It is more
complicated than addition, since all digits of the multiplicand have to be
multiplied by all digits of the multiplier, leading to asymptoticly quadratic
runtimes. Multiple asymptotically faster algorithms exists, such as Karatsuba
and Fast Fourier Transform (FFT), both of which are mentioned by Knuth and part
of GMP \cite{knuth97} \cite{GMP}.

In section \ref{subsec:mulalg}, we present a classical quadratic multiplication
algorithm that convolutes the digits of the operands, and we discuss how to
parallelize it. In sections \ref{subsec:mulcud} and \ref{subsec:mulfut} we
present our CUDA and Futhark implementations, respectively, and any further
optimizations related to the implementations. Penultimately, in section
\ref{subsec:mulsin} we go over the special case of multiplication by a single
precision factor and present an algorithm to handle it. Lastly, in section
\ref{subsec:mulother} we address some assymptoticly better algorithms, namely
the Karatsuba and FFT algorithm, where especially the FFT is promising in a
GPGPU setting.

\subsection{Algorithm}
\label{subsec:mulalg}

The classical multiplication algorithm handles one digit of the multiplier at a
time. E.g. in decimal system, $42\cdot 21$ becomes
$1 \cdot 42 + 2 \cdot 42 \cdot 10$. For our big integers, we have:

\begin{definition}[classical multiplication]\label{def:clasmul}
  Multiplying integer $u\in \mathbb{N}$ by $v \in \mathbb{N}$ in the positional number system of base
  $B$ and $m$ digits, is classicaly computed as:
\begin{equation}
  \label{eq:clasmul0}
  u \cdot v = \sum_{i=0}^{m-1}u_i\left( \sum_{j=0}^{m-1}v_j\cdot B^{j} \right)B^{i}
\end{equation}
\end{definition}

The definition is illustrated in Figure \ref{fig:tiledmult}. We make two
observations: The multiplicand appears inside the outer sum, and we only care
about the first $m$ digits of the result in our setting\footnote{The result of
  multiplication is up to twice the size of the inputs. In order to not loose
  precision in our setting, a memory region of double size should be
  preallocated before the multiplication function call.}. The first observation
means that we cannot compute the multiplication inside the outer sum using
registers, but the second observation lets us redefine the equation exploiting
the tiling clearly visibil in the illustration. Hence, if we use the following
definition, we are both able to compute the inner products using register
arithmetics and avoid unnecessary computations:

\begin{definition}[tiling of classical multiplication preserving input dimensions]\label{def:clasmultil}
  If we are only interrested in the first $m$ digits of the result of Definition
  \ref{def:clasmul}, and we wish to use registers for inner arithmetics, we can
  redefine it:
\begin{equation}
\label{eq:clasmul}
u \cdot v = \sum_{k=0}^{m-1} \left( \sum_{\substack{0\leq i,j < m\\i+j=k}}u_i\cdot v_j \right)B^{k}
\end{equation}
\end{definition}

\begin{figure}
  \centering
  \footnotesize
  \begin{tabular}{c}
    \begin{tabular}{C{0cm}|C{0.8cm}|C{0.8cm}|C{0.8cm}|C{0.5cm}|C{0.8cm}|C{0cm}}
      \cline{2-6}
      & $w_0$ & $w_1$ & $w_2$ & $\ldots$ & $w_{m-1}$ & $\ldots$\\
      \cline{2-6}
    \end{tabular}\\
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $=$ & $=$ & $=$ &  & $=$
    \end{tabular}\\[-2ex]
    \begin{tabular}{C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0cm}C{0.7cm}C{0.2cm}C{0.2cm}C{0.2cm}C{0.2cm}}
      & & $\diagonalarrowdown$ & & $\diagonalarrowdown$ & & $\diagonalarrowdown$ & $\diagonalarrowdown$ & &
    \end{tabular}\\[-1.4ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $0$ & $c_0$ & $c_1$ & $\ldots$ & $c_{m-2}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $+$ & $+$ & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
      $u_0 v_0$ & $u_0 v_1$ & $u_0 v_2$ & $\ldots$ & $u_0v_{m-1}$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & $+$ & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & $u_1v_0$ & $u_1v_1$ & $\ldots$ & $u_1v_{m-2}$ & $\ldots$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & & $+$ &  & $+$
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & & $u_2v_0$ & $\ldots$ & $u_2v_{m-3}$ & $\ldots$
    \end{tabular}\\[-0.6ex]
    \begin{tabular}{C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}}
       & & & $\ddots$ & $\vdots$
    \end{tabular}\\[-0.6ex]
    \begin{tabular}{C{0cm}C{0.8cm}C{0.8cm}C{0.8cm}C{0.5cm}C{0.8cm}C{0cm}}
       & & & & & $u_{m-1}v_0$ & $\ldots$
    \end{tabular}
  \end{tabular}
  \caption{\footnotesize Visualization of classical multiplication given by Definition \ref{def:clasmul}.}
  \label{fig:tiledmult}
\end{figure}

Using this definition, a sequential algorithm is straightforward to
construct. In order to parallelize it, we make three new observations: All of
the tiled inner multiplications are unique (i.e. $O(m^2)$ products must be
computed), the number of terms in the inner sum is linear in $k$ (i.e. the
work), and the overflows can be added as a separate step after the convolutions
(e.g. using \textit{badd}).

Since we parallelize at block-level (i.e. the CUDA block dimensions are $O(m)$),
the first observation tells us that each thread must run $O(m)$ inner products
with no parallelism. Then, from the second observation, we get that letting
thread $t_{k\in\{0,..,m-1\}}$ compute digit $w_k$ gives an unbalanced amount of
sequential work between threads. Instead, we make a fourth observation; the work
of computing digits $w_0$ and $w_{m-1}$ is equal to that of $w_1$ and $w_{m-2}$,
and to that of $w_2$ and $w_{m-3}$, etc.\footnote{This is easy to see on the
  tiled pattern visualized in Figure \ref{fig:tiledmult}}. Thus, by introducing
a fixed sequentialization factor of $2$, we can balance the work amongst
threads, s.t. thread $t_{k\in 0,..,(m-1)/2}$ computes digits $w_k$ and
$w_{m-1-k}$ of the result.

Regarding the third observation, in order to propagate the overflows in a
separate sweep, we must keep track of them while computing the
convolutions. Each thread computes $m+1$ products when the sequentialization
factor is 2. It is a known rule that; given any two factors, their product fit
in their combined precision. E.g. in decimal system, $9$ (precision 1) times
$99$ (precision 2) is $891$ (precision 3). From this rule, each product can fit
in two words. We say the least significant word is the \textit{low part} and the
most significant the \textit{high part}. Since we add $m+1$ products, each part
may overflow at most $m$ times, s.t. low part overflows carry over to the high
part, and high part overflows carry over to the \textit{carry part}. Hence, we
need three words to keep track of each convolution (assuming B is big enough to
hold the carry part).

Thus, we define the parallel algorithm for classical \textit{mul}tiplication by
\textit{conv}olution given in Figure \ref{fig:mulparalg}, from now on called
\textit{convmul}. This algorithm has work $O(m^2)$ and span $O(m)$, where the
sequential convolution is the dominant factor of the span.

\begin{figure}
\begin{lstlisting}[language=pseudo,escapeinside={(*}{*)},frame=single]
fun conv t = -- Parameter `t` represents current index of the m/2 threads
    k1 = t
    k2 = m - 1 - k1            -- The indices `k1` and `k2` represents the
    l1, l2, h1, h2, c1, c2 = 0 -- upper and lower `k` handled by thread `t`

    for i in (0..k1)        -- The indices `i` and `j` are computed
        j = k1 - i          -- straightforward w.r.t. Equation ((*{\hypersetup{allcolors=ForestGreen}\ref{eq:clasmul}}*))
        l1 += u[i] *(*$_{\mathtt{low}}$*) v[j]
        h1 += u[i] *(*$_{\mathtt{high}}$*) v[j] + overflow(*$_{\mathtt{l1}}$*)
        c1 += overflow(*$_{\mathtt{h1}}$*)

    for i in (0..k2)
        j = k2 - i
        l2 += u[i] *(*$_{\mathtt{low}}$*) v[j]
        h2 += u[i] *(*$_{\mathtt{high}}$*) v[j] + overflow(*$_{\mathtt{l2}}$*)
        c2 += overflow(*$_{\mathtt{h2}}$*)

    return (l1, l2, h1, h2, c1, c2)

(l1, l2, h1, h2, c1, c2) = map conv (0..(m/2)-1)
l = concat l1 (reverse l2)           -- The second half of the
h = shift (concat h1 (reverse h2)) 1 -- convolutions are computed in
c = shift (concat c1 (reverse c2)) 2 -- reverse order due to work balancing
r = ADD l h
w = ADD r c
\end{lstlisting}
  \caption{\footnotesize Pseudocode of work balanced parallel algorithm for classical multiplication by convolution of big integers. \texttt{Input:} $u$ and $v$ of size $m$. \texttt{Output:} $w$ of size $m$. \texttt{Use:} Big integer addition function \texttt{ADD}.}
  \label{fig:mulparalg}
\end{figure}

While we cannot optimize the asymptotics of the algorithm, we can eliminate one
of the additions, increasing the parallelism by eliminating a scan.

The intuition is to compute two lower and upper indices of $k$ in Equation
(\ref{eq:clasmul}) per thread, four indices in total. This allows premature
computation of half the additions before the values leave register memory
(i.e. computed by \texttt{conv}), thus, necessitating only one addition after
the convolutions. This optimization also reduces memory usage.

Figure \ref{fig:muloptmem} contains an illustration of the tiling layout of this
optimization w.r.t. threads. Now, the memory layout is trickier to construct
compared to that of Figure \ref{fig:tiledmult}, requiring threads to alternate
writing to the additive arrays. E.g. with four threads ($m = 16$) the addition
corresponds to memory layout:\\

\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
      \hline
      \color{Crimson}$t^0_{00}$ & \color{Crimson}$t^0_{01}$ & \color{Crimson}$t^0_{02}$ & \color{Crimson}$t^0_{03}$ & \color{ForestGreen}$t^2_{00}$ & \color{ForestGreen}$t^2_{01}$ & \color{ForestGreen}$t^2_{02}$ & \color{ForestGreen}$t^2_{03}$ & \color{Chocolate}$t^3_{10}$ & \color{Chocolate}$t^3_{11}$ & \color{Chocolate}$t^3_{12}$ & \color{Chocolate}$t^3_{13}$ & \color{RoyalBlue}$t^1_{10}$ & \color{RoyalBlue}$t^1_{11}$ & \color{RoyalBlue}$t^1_{12}$ & \color{RoyalBlue}$t^1_{13}$\\
      \hline
    \end{tabular}\\
    \begin{tabular}{C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}C{0.41cm}}
      $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$  & $+$ & $+$ & $+$ & $+$
    \end{tabular}\\
    \begin{tabular}{|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|C{0.4cm}|}
      \hline
      0 & 0 & \color{RoyalBlue}$t^1_{00}$ & \color{RoyalBlue}$t^1_{01}$ & \color{RoyalBlue}$t^1_{02}$ & \color{RoyalBlue}$t^1_{03}$ & \color{Chocolate}$t^3_{00}$ & \color{Chocolate}$t^3_{01}$ & \color{Chocolate}$t^3_{02}$ & \color{Chocolate}$t^3_{03}$ & \color{ForestGreen}$t^2_{10}$ & \color{ForestGreen}$t^2_{11}$ & \color{ForestGreen}$t^2_{12}$ & \color{ForestGreen}$t^2_{13}$ & \color{Crimson}$t^0_{10}$ & \color{Crimson}$t^0_{11}$\\
      \hline
    \end{tabular}
  \end{tabular}
\end{center}~

The memory layout perfectly partitions into an addition operator with
sequentialization factor of 4, and thus, the optimization efficiently apply at
CUDA block level, yielding both faster addition\footnote{Experimentally, we
  found addition with sequentialization factor of 4 to be the fastest (see
  section \ref{sec:per}).} and elimination of a scan.

\begin{figure}
  {
  \begin{center}
  \small
  \begin{tabular}{C{0.5cm}}
  \Large{\color{Crimson} $t_0$}~)\\\\
    \Large{\color{RoyalBlue} $t_1$}~)
\end{tabular}
  \begin{tabular}{cccccc}
    \color{Crimson}$l_0$ & \color{Crimson}$h_0$ & \color{Crimson}$c_0$ & & & \\
    & \color{Crimson}$l_1$ & \color{Crimson}$h_1$ & \color{Crimson}$c_1$ & & \\
    & & \color{RoyalBlue}$l_2$ & \color{RoyalBlue}$h_2$ & \color{RoyalBlue}$c_2$ & \\
    & & & \color{RoyalBlue}$l_3$ & \color{RoyalBlue}$h_3$ & \color{RoyalBlue}$c_3$
  \end{tabular}
$\xrightarrow{\text{combines to}}$
\begin{tabular}{cccccc}
     & $\xrightarrow{\scriptsize \text{\color{Crimson}carry}}$ & $\xrightarrow{\scriptsize \text{\color{Crimson}carry}}$ &  &  &  \\
    \color{Crimson}$l_0$ & \color{Crimson}$h_0 + l_1$ & \color{Crimson}$c_0 + h_1$ & \color{Crimson}c$_1$ & & \\
    & & \color{RoyalBlue}$l_2$ & \color{RoyalBlue}$h_2+l_3$ & \color{RoyalBlue}$c_2+h_3$ & \color{RoyalBlue} $c_3$\\
  & & & $\xrightarrow[\scriptsize \text{\color{RoyalBlue}carry}]{}$ & $\xrightarrow[\scriptsize \text{\color{RoyalBlue}carry}]{}$ & \\
\end{tabular}
\end{center}
}

~~~~$\vdots$\qquad\qquad\qquad\qquad\qquad $\ddots$ \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad  $\ddots$
  \caption{\footnotesize Illustration of optimized memory layout of \textit{convmul} with sequentialization factor of 4.}
  \label{fig:muloptmem}
\end{figure}

\subsection{CUDA Implementation}
\label{subsec:mulcud}

In this section, we introduce five CUDA versions, of which two turns out to be
competitive (see Section \ref{sec:per}). The first version (\texttt{V1})
implement the \textit{convmul} algorithm straightforward. The second version
(\texttt{V2}) introduces data type optimization w.r.t. doubling wordsize for
products, and the third version (\texttt{V3}) handles multiple instances per
block to \texttt{V2}. Version four (\texttt{V4}) implements a sequentialization
factor of four according to the optimization described in Section
\ref{subsec:mulalg}, and the fifth (\texttt{V5}) add multiple instances
per block to \texttt{V4}.

We decide the CUDA kernel dimensions and parameters with the following code in
Listing \ref{mulparams}. Akin to Listing \ref{addparams} regarding addition, but
with the blocksize rounded up to $128$ rather than $256$, which we
experimentally found to be faster.

\begin{lstlisting}[language=CPP,caption={\footnotesize CUDA multiplication parameters and kernel dimensions.},label={mulparams}]
const uint32_t q = (v >= 1 && v <= 3) ? 2 : 4;
const uint32_t ipb = (v == 3 || v == 5) ? (128 + (m/q) - 1) / (m/q) : 1;
dim3 block(ipb*(m/q), 1, 1);
dim3 grid (num_instances/ipb, 1, 1);
\end{lstlisting}

The section is divided in three paragraphs, describing the implementations of
convolutions, memory layouts, and multiple instances per block,
respectively. Combined, they describe how we implement the kernels of CUDA
source file \texttt{ker-mul.cu.h}.

\paragraph{Convolutions.}
The implementations use the type \texttt{ubig\_t} of the generic base class,
which is double the base radix (twice the bits of type \texttt{uint\_t}). The
type is used to compute the inner products of the convolution, but it also
allows the optimization of \texttt{V2}. The idea is to postpone the combining of
the low and high parts of the multiplication till after the sequential
loops. Then, the carries can propagate once instead of $i$ times for convolution
$k_i$. The first two convolutions of Listing \ref{cudaconvs} shows the
difference of storing intermediate convolution parts in type \texttt{uint\_t}
(\texttt{V1}) and \texttt{ubig\_t} (\texttt{V2}). The third convolution of
Listing \ref{cudaconvs} shows how to compute and combine two consecutive
convolutions \texttt{(V4)}. It also reveals another upside of this optimization;
we can reuse a digit of $u$, reducing memory usage.

\begin{lstlisting}[language=CPP,escapeinside={(*}{*)},caption={\footnotesize The three convolutions used in the CUDA multiplication kernels of file \texttt{ker-mul.cu.h}.},label={cudaconvs}]
(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k as described in Figure {\hypersetup{allcolors=Crimson}\ref{fig:mulparalg}}}*)
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        uint_t l = (uint_t) uv;
        uint_t h = (uint_t) (uv >> Base::bits);
        // update lows, highs, and carries
        ls += l;
        hs += h + (ls < l);
        cs += hs < (h + (ls < l));
    }

(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k optimized for data type ubig\_t}}*)
    ubig_t l = 0;
    ubig_t h = 0;
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        l += uv & ((ubig_t) Base::HIGHEST);
        h += uv >> Base::bits;
    }
    // update lows, highs, and carries
    ls = (uint_t) l;
    hs = ((uint_t) h) + ((uint_t) (l >> Base::bits));
    cs = ((uint_t) (h >> Base::bits)) + (hs < ((uint_t) h));

(*{\color{Crimson}$\bullet$ \texttt{Convolution over index k and k+1 optimized for ubig\_t and sequentialization factor of 4}}*)
    ubig_t l1 = 0; ubig_t h1 = 0;
    ubig_t l2 = 0; ubig_t h2 = 0;
    for (int i=0; i<=k; i++) {
        // compute high and low part of product
        int j = k - i;
        ubig_t uv1 = ((ubig_t) u[i]) * ((ubig_t) v[j]);
        ubig_t uv2 = ((ubig_t) u[i]) * ((ubig_t) v[j+1]);
        l1 += uv1 & ((ubig_t) Base::HIGHEST);
        h1 += uv1 >> Base::bits;
        l2 += uv2 & ((ubig_t) Base::HIGHEST);
        h2 += uv2 >> Base::bits;
    }
    // remaining computation where i = k+1
    ubig_t uv = ((ubig_t) u[k+1]) * ((ubig_t) v[0]);
    l2 += uv & ((ubig_t) Base::HIGHEST);
    h2 += uv >> Base::bits;
    // update lows, highs, and carries
    ls1 = (uint_t) l1;
    hs1 = ((uint_t) h1) + ((uint_t) (l1 >> Base::bits));
    cs1 = ((uint_t) (h1 >> Base::bits)) + (hs1 < ((uint_t) h1));
    ls2 = (uint_t) l2;
    hs2 = ((uint_t) h2) + ((uint_t) (l2 >> Base::bits));
    cs2 = ((uint_t) (h2 >> Base::bits)) + (hs2 < ((uint_t) h2));
    // combine the lows, highs and carries
    ls = ls1;
    hsls = hs1 + ls2;
    cshs = cs1 + hs2 + (hsls < hs1);
    cscs = cs2 + (cshs < hs2);
\end{lstlisting}

\paragraph{Memory layout.}
Constructing the memory layout of the original algorithm (Figure
\ref{fig:mulparalg}) is straightforward; the shared memory is partitioned in three
equal parts, and each thread write their two convolution parts to each of the
three memory parts. The writes are coalesced by design. E.g. for eight elements
and four threads, we write:
\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}?C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}?C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|C{0.17cm}|}
      \hline
      \color{Crimson}$l^0_{0}$ & \color{RoyalBlue}$l^1_{0}$ & \color{ForestGreen}$l^2_{0}$ & \color{Chocolate}$l^3_{0}$ & \color{Chocolate}$l^3_{1}$ & \color{ForestGreen}$l^2_{1}$ & \color{RoyalBlue}$l^1_{1}$ & \color{Crimson}$l^0_{1}$ & \color{Crimson}$h^0_{0}$ & \color{RoyalBlue}$h^1_{0}$ & \color{ForestGreen}$h^2_{0}$ & \color{Chocolate}$h^3_{0}$ & \color{Chocolate}$h^3_{1}$ & \color{ForestGreen}$h^2_{1}$ & \color{RoyalBlue}$h^1_{1}$ & \color{Crimson}$h^0_{1}$ & \color{Crimson}$c^0_{0}$ & \color{RoyalBlue}$c^1_{0}$ & \color{ForestGreen}$c^2_{0}$ & \color{Chocolate}$c^3_{0}$ & \color{Chocolate}$c^3_{1}$ & \color{ForestGreen}$c^2_{1}$ & \color{RoyalBlue}$c^1_{1}$ & \color{Crimson}$c^0_{1}$ \\
      \hline
    \end{tabular}\\[-0.4ex]
    \begin{tabular}{C{4.45cm}C{4.45cm}C{4.45cm}}
      \upbracefill & \upbracefill & \upbracefill\\[-0.3ex]
      $\mathit{ls}$ & $\mathit{hs}$ & $\mathit{cs}$
    \end{tabular}
  \end{tabular}
\end{center}
Now, reading and adding the three sub-arrays is trivial.

On the other hand, when we optimize the additions for \texttt{V4}, we get a
complicated memory layout (if we are to write/read coalesced). The write is
fairly straightforward, but the read requires more care. E.g. for eight elements and two threads, suppose thread $t^i$ reads four elements to registers $t^i_0$ and four to
$t^i_1$. Then, in order to add the parts correctly, the threads writes and reads in this
pattern:
\begin{center}
  \small
  \begin{tabular}{c}
    \begin{tabular}{C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}C{0.46cm}}
      \color{Crimson}$t^0_{00}$ & \color{Crimson}$t^0_{12}$ & \color{RoyalBlue}$t^1_{00}$ & \color{RoyalBlue}$t^1_{12}$ & \color{Crimson}$t^0_{01}$ & \color{Crimson}$t^0_{13}$ & \color{RoyalBlue}$t^1_{01}$ & \color{RoyalBlue}$t^1_{13}$ & \color{Crimson}$t^0_{02}$ & \color{RoyalBlue}$t^1_{10}$ & \color{RoyalBlue}$t^1_{02}$ & & \color{Crimson}$t^0_{03}$ & \color{RoyalBlue}$t^1_{11}$ & \color{RoyalBlue}$t^1_{03}$ &
    \end{tabular}\\
    \begin{tabular}{|C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}?C{0.45cm}|C{0.45cm}|C{0.45cm}|C{0.45cm}|}
      \hline
      \color{Crimson}$l^0_{0}$ & \color{RoyalBlue}$l^1_{0}$ & \color{RoyalBlue}$l^1_{1}$ & \color{Crimson}$l^0_{1}$ & \color{Crimson}$\mathit{lh}^0_{0}$ & \color{RoyalBlue}$\mathit{lh}^1_{0}$ & \color{RoyalBlue}$\mathit{lh}^1_{1}$ & \color{Crimson}$\mathit{lh}^0_{1}$ & \color{Crimson}$\mathit{hc}^0_{0}$ & \color{RoyalBlue}$\mathit{hc}^1_{0}$ & \color{RoyalBlue}$\mathit{hc}^1_{1}$ & \color{Crimson}$\mathit{hc}^0_{1}$ & \color{Crimson}$\mathit{cc}^0_{0}$ & \color{RoyalBlue}$\mathit{cc}^1_{0}$ & \color{RoyalBlue}$\mathit{cc}^1_{1}$ & \color{Crimson}$\mathit{cc}^0_{1}$ \\
      \hline
    \end{tabular}\\[-0.5ex]
    \begin{tabular}{C{3.2cm}C{3.2cm}C{3.2cm}C{3.2cm}}
      \upbracefill & \upbracefill & \upbracefill & \upbracefill\\[-0.3ex]
      $\mathit{ls}$ & $\mathit{hls}$ & $\mathit{chs}$ & $\mathit{ccs}$
    \end{tabular}
  \end{tabular}
\end{center}~

Appendix Listing \ref{cudamulmem} contains CUDA code for reading and
writing in this pattern.

\paragraph{Multiple instances per block.}
When handling multiple arithmetic instances in a cuda block, e.g. for 4
instances of size 4, we have the following convolution pattern:
\begin{center}
  \begin{tabular}{r}
    \begin{tabular}{|C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \cgray & \cgray &\cgray  &\cgray \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cgray & \cgray & \cgray\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cgray & \cgray\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  & \cgray \\
      \cline{4-4}
    \end{tabular}
  \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
      \cbeige &\cbeige  &\cbeige  &\cbeige \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cbeige & \cbeige & \cbeige\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cbeige &\cbeige \\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  &\cbeige \\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \csteelblue& \csteelblue &\csteelblue  &\csteelblue \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \csteelblue & \csteelblue & \csteelblue\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \csteelblue & \csteelblue\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  & \csteelblue\\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
  \kern-1.4em
  \begin{tabular}{r}
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
      \hline
       \cpurple& \cpurple &\cpurple  &\cpurple \\
      \hline
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}|C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       & \cpurple & \cpurple & \cpurple\\
      \cline{2-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}|C{0.1cm}|C{0.1cm}|}
       &  & \cpurple & \cpurple\\
      \cline{3-4}
    \end{tabular}\\[0.1ex]
    \begin{tabular}{C{0.1cm}C{0.1cm}C{0.1cm}|C{0.1cm}|}
       &  &  &\cpurple \\
      \cline{4-4}
    \end{tabular}
    \end{tabular}
\end{center}
We see that the balancing pattern still applies, i.e., indexing from both
ends is workbalanced.

Thus, we also use this pattern for \texttt{V3} and \texttt{V5}. In turn, the
memory layout also applies. However, we have to be careful when reading from the
memory buffer, as "shifting" the layout is not sufficient get the correct
tiling. Instead, we compute the thread-index w.r.t. segments and, if we are at
the first thread of a segment, we fetch "0" rather than indexing to the previous
segment. The aforementioned Listing \ref{cudamulmem} of the appendix includes
this check.

\subsection{Futhark Implementation}
\label{subsec:mulfut}

The implementation of \textit{convmul} in Futhark has one significant change
from CUDA: There may not be a double-size type for the base type, and so we
cannot always apply the optimization of postponing the carry propagation. In
Futharks high-level design, we do not get all the low-level C primitives, and
that means no efficient 128-bit words.

Hence, the Futhark implementation in base \texttt{u32} use this optimization,
while base \texttt{u64} does not. The first version (\texttt{V1}) implements the
algorithm with sequentialization factor of 2, the second (\texttt{V2}) with a
factor of 4, and the third (\texttt{V3}) allows segmented multiplication.

\subsection{Single Precision Factor}
\label{subsec:mulsin}

Integer multiplication has some special cases; we have multiplication by 0, by 1
and by the radix. Respectively, this is equivalent to 0, identity and a shift
(e.g. $4 \cdot 10^1$ in decimal system is equivalent to $4 \ll 1$ and
$4\cdot 10^2$ to $4\ll 2$ and so forth). Multiplying of big integers has another
special case; multiplication by a single precision factor.

This case can be computed in parallel as an addition with two extra step: I)
Multiply each digit with by the factor giving two arrays with the low and high
parts, respectively. II) Shift the high part array by 1. III) Add the two arrays
using the addition algorithm. Figure \ref{fig:muld} contains the pseudo-code and
illustration of this algorithm.

\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
    \small
    \texttt{Input:} $u$ and of size $m$ base $B$ and digit $d$\\
    \texttt{Output:} $w$ of size $m$ in base $B$\\
    \texttt{Use:} Function \texttt{ADD} for adding big ints
\begin{lstlisting}[language=pseudo,frame=,escapeinside={(*}{*)}]
l = map (*(*$_{\mathtt{low}}$*) d) u
h = map (*(*$_{\mathtt{high}}$*) d) u
h = shift h 1
w = ADD l h
\end{lstlisting}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
    \centering
    \footnotesize
    \begin{tabular}{c}
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $u_0$ & $u_1$ & $u_2$ & $\cdots$ & $u_{m-1}$\\ 
        \hline
      \end{tabular}\\[-0.3ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}}
        $\cdot$ & $\cdot$ & $\cdot$ & & $\cdot$\\ 
      \end{tabular}\\[-0.7ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}C{0.7cm}}
        $d$ & $d$ &  $d$ & $\cdots$ &$d$ \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.8cm}C{0.7cm}C{0.7cm}}
        $=$ & $=$ & $=$ &  & $=$  
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $h_{0}$ & $h_1$ & $h_2$ & $\cdots$ & $h_{m-1}$\\
        \hline
      \end{tabular}\\
      \begin{tabular}{C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}C{0.15cm}}
        \diagonalarrowdown{}$_+$ & & \diagonalarrowdown{}$_+$ &  & \diagonalarrowdown{}$_+$  &  & \diagonalarrowdown{}$_+$ \\
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $l_{0}$ & $l_1$ & $l_2$ & $\cdots$ & $l_{m-1}$\\
        \hline
      \end{tabular}\\
      \begin{tabular}{C{0.7cm}C{0.7cm}C{0.8cm}C{0.7cm}C{0.7cm}}
        $=$ & $=$ & $=$ &  & $=$  
      \end{tabular}\\
      \begin{tabular}{|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|C{0.7cm}|}
        \hline
        $w_{0}$ & $w_1$ & $w_2$ & $\cdots$ & $w_{m-1}$\\
        \hline
      \end{tabular}
    \end{tabular}
  \end{minipage}
  \caption{\footnotesize Pseudo-code and illustration of algorithm for parallel multiplication by single precision.}
  \label{fig:muld}
\end{figure}

We could integrate this into the multiple precision multiplication
implementation, by simply checking whether we are in the special case (or any of
the others for that matter). However there are two problems with this: First, we
expect multiple precision to be more dominant than single precision for a
multiple precision arithmetic library, and so it is not deemed worth the extra
computation. Second, this introduces branching into the multiplication, which is
especially troublesome for working with multiple instances per block (as
diverging branches within a block results in unbalanced work among the threads).

Instead, we write a function for \textit{mul}tiplying a big integer by a
\textit{d}igit, called \textit{muld}. Since it is essentially an addition, we
will not benchmark it or discuss it further.

\subsection{Other Algorithms}
\label{subsec:mulother}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
