\section{Representation of Big Integers}
\label{sec:big}
Big integers, also known as big numbers or multiple-precision integers, are
integers that exceeds the word size on a machine. A common system for reasoning
about them, e.g.\ used by Knuth in \cite{knuth97}, is the positional number
system:

% denoted $\bigint{u_0, u_1,\ldots,u_{m-1}}_{B}$
\begin{definition}[positional number system]\label{def:bigints}
  An integer $u \in \mathbb{N}$ can be expressed in base $B \in \mathbb{N}$ with
  $m$ digits $u_{i\in \{0,1,\ldots,m-1\}}\in\{0,1,\ldots,B-1\}$ by the sum:
  \begin{equation}
\label{eq:rep}
u = \sum_{i=0}^{m-1}u_i\cdot B^{i}
\end{equation}
\end{definition}

By choosing the size of a machine word as the base $B$ (also called the radix),
the positional system maps directly to an array data structure of unsigned
words. I.e.\ for an unsigned word type \uint{} of \texttt{b} bits, we get that a
big integer $u$ of $m$ digits (also called limbs) in base $2^{\mathtt{b}}$ is
represented by an array of type \uint{} and size $m$ in little endian s.t.\
$u\bigint{0} = u_0$, $u\bigint{1} = u_1$, etc. Furthermore, in this
representation, $u$ can be viewed as binary with ${\mathtt{b}\cdot m}$ bits; suppose
a word is 64-bits and the size of the array is 4 -- then $u$ is a 256-bit integer
where, say, the 67th bit of $u$ is the third least significant bit of the second
digit $u\bigint{1}$.

We have to make an important choice between arbitrary precision and exact
precision integer representation. Arbitrary precision, like the name suggests,
means that the precision is not bounded by software, and so we never loose
precision from arithmetic operations (as long as we have enough memory to house
the result). This is for example implemented in Haskell and in GMP
\cite{marlow2010haskell,GMP}. It provides a great deal of abstraction, as there
can occur no overflows. In other words, it allows the programmer to work without
worrying about the sufficiency of the underlying data structure.

However, the cost of this abstraction is the requirement for dynamic memory
allocation to handle overflows. This may be a slight inefficiency on a CPU
architecture and be well worth the level of abstraction, but dynamic memory
management is problematic on a GPU, since all memory sizes must be statically
known, as discussed in section \ref{sec:pre}. Hence, arbitrary precision is an
unsuitable representation for parallel computing.

Exact precision integers are bounded by a specified size, making them more
suitable for a GPU architecture. The size can still be arbitrarily specified and
changed during runtime (by means of allocating and copying to a new big
integer), but the exact size remains known.

Thus, the chosen representation for big integers is exact precision arrays. In
turn, our arithmetics preservers the dimensions between input and output, and
require that the input dimension matches. This setup allows us to fully utilize
GPGPU capabilities.
\newline

So far, we have only considered unsigned integers. There exists multiple
representations of signed integers on a binary machine, the most common being
\textit{two's complement}. This representation does not work well with multiple
precision, as extracting a negative number requires a bitwise negation. Instead,
we can use \textit{sign-magnitude} representation, where a signed integer is
represented by an absolute value along with an indicator bit. The downside of
this representation is that $0$ and $-0$ are two different numbers. This
representation is also used in other high-efficiency libraries, such as GMP
\cite{GMP}.

We use unsigned integers as the internal representation, as this is more
efficient. In cases where signs are required, we extent by
sign-magnitude. Furthermore, we only consider unsigned arithmetics (with
wrap-around on overflow), as signed arithmetics are trivial to define using
their unsigned counterpart.


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
