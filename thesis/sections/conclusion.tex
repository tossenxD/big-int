\section{Conclusion}
\label{sec:conc}
This thesis has shown how to implement exact and efficient big integer
arithmetic on a GPU. The focus has been on addition, multiplication, and
division, using the high-level programming languages Futhark and \cpp. The
arithmetic operators have been implemented at CUDA block-level, albeit
algorithmic efforts have been kept general and can be fitted for other
architectures. The arithmetic operators are aimed at medium-sized big integers.
We have shown how to represent such integers in a GPGPU friendly way and more,
what optimization strategies give consistent performance over the shape of the
integers.

\pagebreak

Regarding the addition operator, we found that carry propagation in parallel can
be expressed using a scan, making it efficiently run on GPGPU. Our
multiplication operator is based on the classical multiplication algorithm. The
algorithm displays a quadratic amount of sequential work, yet we have shown a
scheme that balances the work amongst threads and maximize performance at CUDA
block-level. The scheme contains a nontrivial memory layout as a byproduct of the
way multiplication convolutions are arranged. We have shown how to efficiently
construct the layout, both on paper and in implementation.

The approach for the division operator is based on multiplication by the inverse
divisor. We leverage the existence of a shift operator to stay in the domain of
big integers, such that it becomes multiplication by a shifted inverse. The
approach is grounded in another work detailing an algorithm to find such a
shifted inverse. We have adapted the algorithm to big integers, while also
expanding on it in the form of handling an otherwise unconsidered cornercase. We
have shown how to use this algorithm in order to efficiently find the exact
remainder and quotient of two big integers. The division is parameterized over a
big integer multiplication, addition, and subtraction method, and its complexity
mirrors that of its multiplication method. While we were not able to produce a
correct and efficient implementation of the algorithm for GPGPU, the necessary
steps to do so have been outlined. Some of the steps have been implemented,
including a sign-extended parallel subtraction operator based on our addition
operator. Moreover, a correct but inefficient sequential C implementation has
been produced, serving as proof of concept for the division algorithm.

Our addition and multiplication operator have been tested against a state of the
art CUDA library for big integers. We found that the block-level approach
overall exhibit good performance at the target size. Both operators have better
performance than the state of the art library when run once, but consecutively
applying the operators scales worse and comes with a performance cost. For
addition, our \cpp\ implementation is still competitive given this performance
cost, while our Futhark implementation becomes slow. For multiplication, our
Futhark implementation suffers from an inefficient subroutine, along with not
supporting all of the low-level data types that \cpp\ supports, and thus, cannot
be as optimized. Our \cpp\ implementation is still competitive at the target
integer size, given the performance cost that comes with scaling the number of
arithmetic operators applied.

\paragraph{Future work}
This thesis leaves future work regarding the division implementation. First and
foremost, the parallel implementation should be validated. In relation to that,
it should be considered whether the increased number of guard digits is a true
revision to the algorithm, or due to an error in the sequential prototype. After
it validates, the remaining inefficient subroutines should be reworked, mainly
ensuring proper growth of the multiplication input lengths over refinement
iterations. The last step is to evaluate its performance over big integer sizes,
divisor precision, and the underlying multiplication methods, such as FFT and
classical.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
