\section{Introduction}
\label{sec:intro}

Integers are commonly represented as either 32- or 64-bit in both hardware and
software. They are an essential part of computing and we expect most programs to
utilize integer arithmetics in accomplishing of a diverse range of
tasks. However, some applications require numbers that are too big to fit a
64-bit integer. One solution is resorting to floating-point arithmetics, but
those are both imprecise and inefficient compared to integer arithmetics.

Exact arithmetics with big integers (also called multiple precision integers or
big numbers) is the very foundation of numerous fields in computer science,
other formal sciences, natural sciences, and industry. Evident examples includes
cryptography and algebra. Big integers can span hundreds, thousands, or even
millions of bits, necessitating the exact arithmetics to be efficient in the
size of the integers. A widespread implementation for such arithmetics is the
GNU Multiple Precision Arithmetic Library (GMP) written in C and assembly
\cite{GMP}. One approach to further accelerate the performance is utilizing
massively parallel hardware such as General Purpose Graphics Processing Units
(GPGPUs).

In order to efficiently use GPGPUs, the underlying algorithms have to be adapted
and parallelized. Addition has shown to be very efficient, running in sub-linear
time \cite{DPPproject,blellochaddscan}. Multiplication classically runs in
quadratic time \cite{knuth97}. The classical approach adapted to GPGPU is found
to be efficient for small- and medium-sized big integers
\cite{doi:10.1177/10943420221077964, oancea2024gpu}. Fast Fourier Transform
(FFT) based multiplication algorithms are known to be asymptotically faster
\cite{knuth97}. Due to the overhead of FFT, such approaches are most efficient
on GPGPU for large-sized big integers in comparison to the classical approach
\cite{Bantikyan2014BigIM, doi:10.1177/10943420221077964, oancea2024gpu}.

Division is the hardest of the basic arithmetics. It traditionally involves a
long division algorithm that iteratively finds one correct digit
\cite{knuth97}. With the number of iterations linear in the input size, this
algorithm is a poor fit for GPGPU. Another common division approach is by
multiplicative inverses. Watt has shown an algorithm to efficiently compute
exact division by finding such an inverse, without leaving the original domain
\cite{watt2023efficient}. Its complexity mirrors that of its multiplication
method, which can be parameterized, and the number of iterations is logarithmic
in the input size, yielding a more GPGPU suitable algorithm. \bigskip

This Thesis focus on efficient parallel implementations of exact big integer
arithmetics for GPGPU. It presents the algorithms for sub-linear addition,
classical multiplication, and Watt's exact division by whole shifted
inverse. Algorithmic and parallelization efforts are kept general, but the
implementations are narrowed to the Compute Unified Device Architecture (CUDA)
platform through the programming languages C++ and Futhark. Both are high-level
languages, but operates vastly different. C++ allows low-level command of
primitives and fine-grained memory control, while interfacing directly with the
CUDA runtime API to produce GPGPU executable code \cite{cudaguide,
  stroustrup}. Futhark is a functional array programming language that is
designed around parallel basic blocks, making programs more elegant and less
dependant on hardware specifications in exhange for loosing some of the
fine-grained and low-level control \cite{ParallelProgrammingInFuthark,
  Henriksen:2017:FPF:3062341.3062354}.

The arithmetics are implemented at CUDA block-level, and hence, aimed at
medium-sized big integers ranging roughly from a few hundred to a hundred
thousand bits. Each algorithm includes optimizations to further enhance the
performance at block-level -- or performance in general. The results shows that
both the produced addition and multiplication methods are competitive
performance-wise, but the performance gap between C++ and Futhark implementations
grows with the complexity of the algorithms and applied optimizations.

The produced implementations of division are not as highly optimized or
efficient as for the other arithmetics. However, this Thesis is (to our
knowledge) the first to recognise and use Watt's division algorithm (outside of
Watt's own work), and in turn, first to parallelize it.

The contributions of this Thesis are:
\begin{itemize}
\item An extendable representation of big integers as an array data structure
  over a generic base type that allows fast arithmetics on GPGPU, along with
  implementations in C++ and Futhark using classes and type declarations.
\item A description of efficient parallel big integer addition and classical
  multiplication algorithms, on top of gradual degrees of optimizations over the
  shape of the inputs, accompanied by implementations thereof at CUDA
  block-level in C++ and Futhark.
\item A presentation of the high-level intuition and a specialization for big
  integers of Watt's algorithm for exact division by whole shifted inverse,
  including a revision that extends the algorithm to an otherwise unconsidered
  corner case.
\item An inefficient sequential prototype of Watt's algorithm in a low-level
  language (C).
\item A parallelization effort of Watt's algorithm that culminates in a
  partially valid and semi-efficient Futhark implementation, entailing efficient
  parallel operators for shifting, comparing, and subtracting big integers.
\end{itemize}
\bigskip

The structure of this Thesis is as follows: Section \ref{sec:rel} presents other
works related to the subject of this Thesis. Section \ref{sec:software} details
the practicalities of the software suite that has been developed as part of this
Thesis. Section \ref{sec:pre} provides the background information on GPGPU, CUDA
C++, and Futhark that is assumed throughout this Thesis. Section \ref{sec:big}
addresses the representation of big integers. Section \ref{sec:strat} outlines
the overarching strategy of the implementations. Sections \ref{sec:add},
\ref{sec:mul}, and \ref{sec:div} presents the algorithms, optimizations and
implementations of addition, multiplication, and division, respectively. Section
\ref{sec:cor} presents the methodology and results of the validation tests for
the arithmetics. Section \ref{sec:per} benchmarks and evaluates the performance
of the developed addition and multiplication implementations, while giving a
detailed description of the methodology and metrics behind the
benchmarks. Section \ref{sec:conc} concludes the work of this Thesis and lists
directions for future works.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
