\section{Performance}
\label{sec:per}

To assess the efficiency of our implementations, we measure their relative
performance of running a benchmark suite. We expect that the CUDA
implementations yields the best performance given the underlying algorithms and
optimizations, and in turn, reveals the overhead and inefficiencies of the
Futhark implementations. We also run the benchmarks using the CGBN library
described in section \ref{sec:rel}. It enlights the relative performance of our
implementations against state of the art, in addition to the strong and weak
points of the underlying algorithms and overarching implementations strategy
(e.g.\ pros and cons of block-level versus warp-level processing of big
integers).

The runtimes of benchmarks can be tedious to directly compare. Instead, we
define some performance metrics based on the runtime, total number of bits, and
arithmetic operation, allowing us to directly compare results. We keep the total
number of bits fixed, but vary the size of integers and number of integer
instances across runs.

The structure is as follows: Section \ref{subsec:benchset} describes the setup
of our benchmarks, section \ref{subsec:perfmet} introduces the performance
metrics, and section \ref{subsec:benchres} presents benchmark results.

\subsection{Benchmark Setup}
\label{subsec:benchset}

We have two benchmark types: The first type runs the arithmetic operators
straightforward, serving as the basis for evaluating performance. The second
type runs multiple consecutive applications of the operators, providing insight
on how well the arithmetics scales. The consecutive calls in CUDA are collected
in a single kernel to reduce memory overhead by keeping intermediate results in
shared memory. In Futhark we have consecutive intra-block calls, benchmarking
the compilers ability to fuse the arithmetics compared to CUDA. The consecutive
calls of the second type corresponds to computing $10(u+v)$ and $(u\cdot
v)^5$. Therefore, it is ten additions and six multiplications.

While we want to vary the size and number of big integers, the benchmark results
are only directly comparable if we keep the total amount of work fixed. E.g.\
consider a fixed work size of 256 bits; the benchmarks can be run with $n=1$
instance of base 32-bit big integers of size $m=8$, or, run with $n=1$, $m=4$,
and base 64-bit, or, run with $n=2$, $m=2$, and base 64-bit, etc.\ -- which all
uses the same number of bits in total. We choose the fixed total amount of work
to be $2^{32}$-bits, i.e.\ $2^{27}$ $\mathtt{u32}$-words or $2^{26}$
$\mathtt{u64}$-words. We measure across the sizes of
$\{2^9,2^{10},\ldots,2^{18}\}$-bits big integers, and adjust the number of instances
accordingly.

Runtimes are averaged over multiple runs to improve stability and accuracy. In
CUDA we average across 300 runs for addition and 100 runs for multiplication,
with one dry-run before starting the timer. We transfer memory to the device
beforehand and stop the timer as soon as the kernel has completed its runs. In
Futhark we use the \texttt{futhark-bench} utility, that allows us to write
benchmark-specifications, similarly to \texttt{futhark-test}
\cite{futguide}. This utility also handles memory and do a dry-run before
starting the timer. It runs until the confidence level has reached 95\% (with at
least 10 runs) and reports both the average runtimes and the confidence
interval.

\subsection{Performance Metrics}
\label{subsec:perfmet}

Metrics are defined on an operator basis and derived from the complexities,
inner workings, and benchmark setup of the operators. Runtimes are measured in
microseconds.

\paragraph{Addition}
In section \ref{subsec:addalg} we analyze the complexity of addition to work
$O(m)$ and depth $O(\log m)$. It is computationally efficient, and only require
communication w.r.t.\ the prefix sum. Hence, we expect addition to be
\textit{bandwidth} bound rather than \textit{operation} bound.

For $n$ instances of addition of big integers with size $m$, we calculate the
bandwidth (measured in GB/s) using the following formula:
\begin{equation}
    \label{eq:bandwidth}
    \mathit{bandwidth} = 3 \cdot \dfrac{n \cdot m \cdot (\mathtt{bits}/8)}{1000000000} \cdot \dfrac{1000000}{\mathit{runtime}}
\end{equation}
The first fraction represents how many gigabytes are in an array of big
integers, where the term $\mathtt{bits} / 8$ is the number of bytes in a word,
$n\cdot m$ is the total amount of words, and the denominator converts from bytes to
gigabytes. The second fraction is the runtime converted to seconds. The constant
3 is because addition must access global memory thrice -- twice for reading the
inputs and once for writing the output -- regardless of performing one or ten
consecutive additions, since intermediate results can remain in shared
memory. Thus, we use the same formula for both benchmark setups.


\paragraph{Multiplication}
In section \ref{subsec:mulalg} we analyze the complexity of classical
multiplication to work $O(m^2)$ and depth $O(m)$. It is a computationally
demanding operation, and hence, we expect multiplication to be
\textit{operation} bound.

While we could center the metric around the number of \texttt{uint\_t}
operations, it leads to circuitous comparisons of benchmarks results across
bases. Instead, we normalize the results by choosing the number of
\texttt{uint32\_t} operations as the operational core for our metric. We then
define the metric as the number of gigaoperations per second (\textit{Gu32ops}).

The metric us not as clear-cut as the bandwidth, but we can estimate it. For
classical multiplication, the number of operations can be estimated by the
squared number of 32-bit words times the number of instances:
\begin{equation}
  \label{eq:u32opsobs}
  {n \cdot (m \cdot (\mathtt{bits} / 32))^2}
\end{equation}

However, classical multiplication is not asymptotically optimal. Instead, we
base our metric upon FFT multiplication, on the ground that it should reflect
the optimal scenario. We use the one proposed by Oancea and Watt in
\cite{oancea2024gpu}:
\begin{equation}
  \label{eq:u32ops}
  \mathit{Gu32ops} = \dfrac{n \cdot 300 \cdot (m \cdot (\mathtt{bits} / 32)) \cdot \log (m \cdot (\mathtt{bits} / 32))}{1000000000} \cdot \dfrac{1000000}{\mathit{runtime}}
\end{equation}

When we compute six consecutive multiplications, we execute six times the number
of operations, and thus, we multiple the metric by 6 in these benchmarks.

\subsection{Benchmark Results}
\label{subsec:benchres}

We report and discuss the benchmark results for addition and multiplication in
sections \ref{subsubsec:addres} and \ref{subsubsec:mulres}. Section
\ref{subsubsec:sumres} summarize the discussions. Regarding the structure of the
benchmark results tables: Legends starting with \textit{C-} refers to CUDA
implementations, and \textit{F-} refers to Futhark implementations. The legend
\textit{CGBN} shows the result of running the benchmark with CGBN, and
\textit{Bits} and \textit{Instances} contains the batch and integer size for the
benchmarks. Entries denoted as ` -- ` refuse to run (e.g.\ exceeds
block-level). The benchmarks are run on a NVIDIA GTX 1650 SUPER -- a GPU with
1280 CUDA cores, 4GB memory, 192.0 GB/s memory bandwidth, and 4.416 TFLOPS
compute power \cite{gpuspecs}.

\subsubsection{Addition Results}
\label{subsubsec:addres}

Tables \ref{add1u64} and \ref{add1u32} shows benchmarks for one addition in
64-bit and 32-bit base, respectively, while Tables \ref{add10u64} and
\ref{add10u32} shows for ten additions. Here is what we can gather from the
tables:
\begin{itemize}
  \renewcommand\labelitemi{--}
\item \textbf{Implementation versions} agrees that \texttt{V3} has best
  performance (which is also the most optimized version). The only outliers are
  that for 10 additions, the segmented scan has a slight cost at around
  $2^{15}$-bits, with biggest difference in bandwidth being 73\% and 65\% peak
  bandwidth in Table \ref{add10u64} for $2^{16}$-bit integers of base
  \texttt{u64}. We only consider \texttt{V3} in the following observations.
\item \textbf{Choice of base for one addition} in CUDA is inconsequential until
  the inputs are of $2^{18}$-bits. This is where base \texttt{u32} results in
  sequentialization factor $q=8$, while \texttt{u64} remains at $q = 4$. In
  Futhark, the difference is more noticeable, where \texttt{u64} is around 88\%
  of the peak bandwidth, and \texttt{u32} around 82\%. As in the CUDA case,
  \texttt{u64} is more suitable for higher bit count.
\item \textbf{Choice of base for ten addition} in CUDA is impactful, where
  \texttt{u32} runs at roughly 40\% peak bandwidth, and \texttt{u64} at roughly
  64\%. For Futhark, the difference is still there, but more subtle at 13\% and
  15\%.
\item \textbf{Choice of language (Futhark or CUDA)} is inconsequential for one
  addition of base \texttt{u64}, but CUDA has a slight advantage in base
  \texttt{u32} and are able to maintain 88\% peak bandwidth, whereas Futhark
  drops to 82\% peak bandwidth. However, the difference in the two languages are
  much more noticeable for ten additions. The biggest difference is that of
  Table \ref{add10u64}, where CUDA runs at 65\% peak bandwidth and Futhark at
  15\%.
\item \textbf{Compared to CGBN} we see that our implementations run faster for
  all benchmark sizes when computing one addition, except for $2^{11}$ to
  $2^{13}$ bits. In this interval, CGBN peaks and all implementations utilize
  roughly the same amount of bandwidth. However, when moving to ten additions,
  CGBN is able to maintain its bandwidth utilization, whereas our
  implementations (especially Futhark) shows a big performance drop. As
  discussed in section \ref{sec:rel}, this is due to CGBN working at warp-level
  rather than block-level, reducing the latency overhead of running consecutive
  arithmetics. However, our CUDA addition is still faster than CGBN for sizes
  $2^{14}$-bits and up. Another noticeable different is the consistency of
  bandwidth utilization in our best implementations compared to CGBN.
\end{itemize}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 62  & --   & 147 & 161 & --   & --   & --   & --   \\\hline
    $2^{17}$ & $2^{15}$ & 67  & --   & 161 & 163 & --   & --   & --   & --   \\\hline
    $2^{16}$ & $2^{16}$ & 19  & 168 & 166 & 166 & 116 & 155 & 158 & 146 \\\hline
    $2^{15}$ & $2^{17}$ & 19  & 168 & 167 & 166 & 146 & 168 & 168 & 168 \\\hline
    $2^{14}$ & $2^{18}$ & 84  & 168 & 168 & 166 & 150 & 168 & 168 & 168 \\\hline
    $2^{13}$ & $2^{19}$ & 164 & 168 & 168 & 165 & 158 & 168 & 168 & 168 \\\hline
    $2^{12}$ & $2^{20}$ & 165 & 168 & 168 & 166 & 157 & 168 & 142 & 168 \\\hline
    $2^{11}$ & $2^{21}$ & 164 & 169 & 162 & 166 & 134 & 165 & 75  & 168 \\\hline
    $2^{10}$ & $2^{22}$ & 156 & 107 & 92  & 166 & 72  & 91  & 38  & 168 \\\hline
    $2^{9}$  & $2^{23}$ & 118 & 55  & 47  & 167 & 36  & 47  & 20  & 168 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of one addition in base \texttt{u64} measured in GB/s (higher is better, 192 is peak).}
  \label{add1u64}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 62  & --   & 104 & 100 & --  & --   & --   & --   \\\hline
    $2^{17}$ & $2^{15}$ & 67  & --   & 168 & 168 & --  & --   & --   & --   \\\hline
    $2^{16}$ & $2^{16}$ & 19  & --   & 168 & 168 & --  & --   & 124 & 115 \\\hline
    $2^{15}$ & $2^{17}$ & 19  & 136 & 168 & 168 & 61 & 83  & 165 & 153 \\\hline
    $2^{14}$ & $2^{18}$ & 84  & 163 & 168 & 168 & 76 & 112 & 169 & 157 \\\hline
    $2^{13}$ & $2^{19}$ & 164 & 169 & 168 & 168 & 79 & 115 & 169 & 158 \\\hline
    $2^{12}$ & $2^{20}$ & 165 & 169 & 168 & 168 & 84 & 122 & 145 & 157 \\\hline
    $2^{11}$ & $2^{21}$ & 164 & 155 & 159 & 168 & 83 & 121 & 77  & 157 \\\hline
    $2^{10}$ & $2^{22}$ & 156 & 105 & 92  & 168 & 72 & 91  & 39  & 158 \\\hline
    $2^{9}$  & $2^{23}$ & 118 & 54  & 47  & 168 & 37 & 47  & 20  & 158 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of one addition in base \texttt{u32} measured in GB/s (higher is better, 192 is peak).}
  \label{add1u32}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 25  & --  & 108 & 92  & --  & --  & --  & --  \\\hline
    $2^{17}$ & $2^{15}$ & 60  & --  & 120 & 109 & --  & --  & --  & --  \\\hline
    $2^{16}$ & $2^{16}$ & 73  & 40 & 142 & 124 & --  & --  & 27 & 24 \\\hline
    $2^{15}$ & $2^{17}$ & 45  & 47 & 137 & 124 & 15 & 24 & 34 & 29 \\\hline
    $2^{14}$ & $2^{18}$ & 97  & 50 & 113 & 124 & 19 & 27 & 32 & 29 \\\hline
    $2^{13}$ & $2^{19}$ & 162 & 45 & 82  & 123 & 19 & 31 & 30 & 29 \\\hline
    $2^{12}$ & $2^{20}$ & 164 & 35 & 44  & 123 & 17 & 29 & 20 & 29 \\\hline
    $2^{11}$ & $2^{21}$ & 161 & 24 & 23  & 124 & 15 & 25 & 11 & 29 \\\hline
    $2^{10}$ & $2^{22}$ & 152 & 12 & 12  & 124 & 8  & 13 & 5  & 29 \\\hline
    $2^{9}$  & $2^{23}$ & 113 & 6  & 6   & 124 & 4  & 7  & 3  & 29 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of ten additions in base \texttt{u64} measured in GB/s (higher is better, 192 is peak).}
  \label{add10u64}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 25  & --  & 75 & 76 & --  & --  & --  & --  \\\hline
    $2^{17}$ & $2^{15}$ & 60  & --  & 66 & 55 & --  & --  & --  & --  \\\hline
    $2^{16}$ & $2^{16}$ & 73  & --  & 81 & 68 & --  & --  & 23 & 21 \\\hline
    $2^{15}$ & $2^{17}$ & 45  & 21 & 88 & 77 & 9  & 14 & 25 & 23 \\\hline
    $2^{14}$ & $2^{18}$ & 97  & 24 & 79 & 77 & 9  & 17 & 26 & 24 \\\hline
    $2^{13}$ & $2^{19}$ & 162 & 25 & 62 & 77 & 10 & 18 & 23 & 24 \\\hline
    $2^{12}$ & $2^{20}$ & 164 & 23 & 44 & 77 & 10 & 17 & 20 & 24 \\\hline
    $2^{11}$ & $2^{21}$ & 161 & 17 & 23 & 77 & 9  & 16 & 11 & 24 \\\hline
    $2^{10}$ & $2^{22}$ & 152 & 12 & 11 & 77 & 8  & 13 & 6  & 24 \\\hline
    $2^{9}$  & $2^{23}$ & 113 & 6  & 6  & 77 & 4  & 7  & 3  & 24 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of ten additions in base \texttt{u32} measured in GB/s (higher is better, 192 is peak).}
  \label{add10u32}
\end{table}

\pagebreak

\subsubsection{Multiplication Results}
\label{subsubsec:mulres}

Tables \ref{mul1u64} and \ref{mul1u32} shows the results for one multiplication
in 64-bit and 32-bit base, respectively, while Tables \ref{mul6u64} and
\ref{mul6u32} shows for six multiplications. We can gather from the tables:
\begin{itemize}
  \renewcommand\labelitemi{--}
\item \textbf{Implementation versions} in CUDA agree that \texttt{V5} is best,
  both for one and six multiplications, as expected. The importance of
  processing multiple instances per block for sizes $2^9$ to $2^{11}$ bits is
  very noticeable on the two versions supporting this (\texttt{V3} and
  \texttt{V5}). W.r.t.\ Futhark, the inefficiencies of \texttt{V2} and
  \texttt{V3} described in section \ref{subsec:mulfut} has a huge impact on the
  performance. E.g.\ in Table \ref{mul1u32} with $2^{12}$-bit integers,
  \texttt{V1} gives roughly 4$\times$ more Gu32ops than \texttt{V3}. However, we
  still see the pattern that handling multiple instances per block is important
  for efficiency at sizes $2^9$ to $2^{11}$ bits.
\item \textbf{Choice of base for one multiplication} strongly favors running in
  64-bit base. The average increase in Gu32ops of using \texttt{u64} compared to
  \texttt{u32} is in CUDA \texttt{V5} 1.4$\times$ and in Futhark \texttt{V1}
  1.9$\times$. This is expected, since halving the base size will double the number
  of digits, and the classical multiplication is quadratic in the number of
  digits.
\item \textbf{Choice of base for six multiplications} also favors \texttt{u64},
  but with a smaller factor of Gu32ops than for one multiplication, e.g.\
  comparing base \texttt{u64} to \texttt{u32} for CUDA \texttt{V5} is
  1.2$\times$ and for Futhark \texttt{V1} it is 1.7$\times$ on average.
\item \textbf{Choice of language (Futhark or CUDA)} has a big performance
  impact. Futhark does not have 128-bit integer support, which is a suspect for
  the gap between the two languages. Even if we compare \texttt{V1} in CUDA and
  Futhark (i.e.\ with no extra optimizations added to \textit{convmul}), we
  still see and average increase of Gu32ops for CUDA compared to Futhark of:
  1.6$\times$ in Table \ref{mul1u64}, 2.2$\times$ in Table \ref{mul1u32},
  1.7$\times$ in Table \ref{mul6u64}, and 2.7$\times$ in Table \ref{mul6u32}. However, the
  implementations of both languages seems to roughly follow the same pattern
  w.r.t.\ the number of bits and instances, which could indicate that if the gap
  can be closed (e.g.\ if Futhark gets 128-bit integers), the Futhark
  implementation will become competitive as well.
\item \textbf{Compared to CGBN}, both our best CUDA and Futhark implementation
  is much faster for one multiplication. However, for six multiplications, CGBN
  is more efficient than ours up to $2^{16}$ bit integers. For smaller integers
  (i.e.\ of $2^9$ to $2^{11}$ bits), CGBN is \textit{much} faster than our
  implementations when running six multiplications -- while for $2^{14}$ and
  $2^{15}$ bits integers, the performance gap between our implementations and
  CGBN is less significant.
\end{itemize}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3} & C-\texttt{V4} & C-\texttt{V5} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & --   & --    & --    & --     & --    & --     & --    & --    & --    \\\hline
    $2^{17}$ & $2^{15}$ & 1   & 836  & 1131 & 829   & 1167 & 1150  & --    & --    & --    \\\hline
    $2^{16}$ & $2^{16}$ & 35  & 1491 & 1997 & 1542  & 2055 & 2039  & 974  & --    & --    \\\hline
    $2^{15}$ & $2^{17}$ & 116 & 2578 & 3367 & 2719  & 3512 & 3471  & 1674 & 469  & 482  \\\hline
    $2^{14}$ & $2^{18}$ & 217 & 4239 & 5326 & 4476  & 5640 & 5515  & 2671 & 652  & 693  \\\hline
    $2^{13}$ & $2^{19}$ & 340 & 6425 & 7530 & 6722  & 7968 & 8082  & 3880 & 1035 & 984  \\\hline
    $2^{12}$ & $2^{20}$ & 526 & 8159 & 8343 & 9180  & 6981 & 10475 & 4931 & 1448 & 1281 \\\hline
    $2^{11}$ & $2^{21}$ & 793 & 6059 & 5575 & 10749 & 5572 & 15745 & 3836 & 1551 & 1899 \\\hline
    $2^{10}$ & $2^{22}$ & 822 & 3700 & 3238 & 12990 & 3909 & 16554 & 2352 & 1130 & 2492 \\\hline
    $2^{9}$  & $2^{23}$ & 496 & 1787 & 1722 & 13740 & 2225 & 16888 & 1122 & 704  & 2798 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of one multiplication in base \texttt{u64} measured in Gu32ops (higher is better).}
  \label{mul1u64}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3} & C-\texttt{V4} & C-\texttt{V5} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & --   & --    & --    & --    & --    & --     & --    & --   & --    \\\hline
    $2^{17}$ & $2^{15}$ & 1   & --    & --    & --    & 699  & 699   & --    & --   & --    \\\hline
    $2^{16}$ & $2^{16}$ & 35  & 893  & 1195 & 797  & 1258 & 1258  & --    & --   & --    \\\hline
    $2^{15}$ & $2^{17}$ & 116 & 1564 & 2074 & 1408 & 2229 & 2226  & 647  & --   & --    \\\hline
    $2^{14}$ & $2^{18}$ & 217 & 2619 & 3342 & 2378 & 3774 & 3773  & 1090 & 242 & 247  \\\hline
    $2^{13}$ & $2^{19}$ & 340 & 4203 & 4889 & 3767 & 5961 & 6061  & 1683 & 352 & 395  \\\hline
    $2^{12}$ & $2^{20}$ & 526 & 5975 & 6117 & 5412 & 8350 & 8815  & 2300 & 513 & 553  \\\hline
    $2^{11}$ & $2^{21}$ & 793 & 6270 & 6041 & 6728 & 7305 & 11363 & 2781 & 738 & 675  \\\hline
    $2^{10}$ & $2^{22}$ & 822 & 3704 & 3355 & 7238 & 4342 & 11448 & 2069 & 803 & 930  \\\hline
    $2^{9}$  & $2^{23}$ & 496 & 1731 & 1585 & 8112 & 2138 & 12979 & 1117 & 580 & 1107 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of one multiplication in base \texttt{u32} measured in Gu32ops (higher is better).}
  \label{mul1u32}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3} & C-\texttt{V4} & C-\texttt{V5} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & --     & --    & --    & --     & --    & --     & --    & --    & --    \\\hline
    $2^{17}$ & $2^{15}$ & 11    & --    & --    & --     & --    & --     & --    & --    & --    \\\hline
    $2^{16}$ & $2^{16}$ & 888   & 1337 & 1775 & 1259  & 1718 & 1747  & 921  & --    & --    \\\hline
    $2^{15}$ & $2^{17}$ & 2832  & 2414 & 2081 & 2569  & 2229 & 2602  & 1595 & 357  & 350  \\\hline
    $2^{14}$ & $2^{18}$ & 4960  & 3730 & 1746 & 4162  & 2682 & 2696  & 1656 & 528  & 513  \\\hline
    $2^{13}$ & $2^{19}$ & 8625  & 4205 & 4630 & 6293  & 5094 & 4961  & 1872 & 806  & 778  \\\hline
    $2^{12}$ & $2^{20}$ & 13924 & 7557 & 6537 & 8609  & 4374 & 8981  & 3307 & 1217 & 1029 \\\hline
    $2^{11}$ & $2^{21}$ & 23424 & 4889 & 4482 & 10084 & 3721 & 13717 & 3028 & 979  & 1505 \\\hline
    $2^{10}$ & $2^{22}$ & 37500 & 3140 & 2741 & 12848 & 3026 & 17513 & 2180 & 899  & 1946 \\\hline
    $2^{9}$  & $2^{23}$ & 70093 & 1641 & 1522 & 14243 & 2024 & 17079 & 1225 & 728  & 2156 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of six multiplications in base \texttt{u64} measured in Gu32ops (higher is better).}
  \label{mul6u64}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3} & C-\texttt{V4} & C-\texttt{V5} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & --     & --    & --    & --    & --    & --     & --    & --   & --    \\\hline
    $2^{17}$ & $2^{15}$ & 11    & --    & --    & --    & --    & --     & --    & --   & --    \\\hline
    $2^{16}$ & $2^{16}$ & 888   & 878  & 1192 & 771  & 1275 & 1268  & --    & --   & --    \\\hline
    $2^{15}$ & $2^{17}$ & 2832  & 1539 & 2086 & 1376 & 2236 & 2235  & 626  & --   & --    \\\hline
    $2^{14}$ & $2^{18}$ & 4960  & 2622 & 3379 & 2348 & 3337 & 3082  & 1056 & 192 & 190  \\\hline
    $2^{13}$ & $2^{19}$ & 8625  & 4024 & 4023 & 3342 & 4948 & 3841  & 1217 & 288 & 307  \\\hline
    $2^{12}$ & $2^{20}$ & 13924 & 4789 & 5417 & 4719 & 7723 & 6522  & 1068 & 422 & 441  \\\hline
    $2^{11}$ & $2^{21}$ & 23424 & 5407 & 6220 & 5575 & 6737 & 10481 & 1874 & 544 & 555  \\\hline
    $2^{10}$ & $2^{22}$ & 37500 & 3517 & 3393 & 6524 & 4467 & 12414 & 2123 & 496 & 816  \\\hline
    $2^{9}$  & $2^{23}$ & 70093 & 1771 & 1640 & 7528 & 2396 & 15135 & 1224 & 477 & 1062 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of six multiplications in base \texttt{u32} measured in Gu32ops (higher is better).}
  \label{mul6u32}
\end{table}

\subsubsection{Summary of Results}
\label{subsubsec:sumres}
Overall, the implementations we have produced are generally competitive in the
integer size range of $2^{14}$ to $2^{16}$ bits. When only calling one
arithmetic at a time, our implementations are consistently faster than
CGBN. However, CGBN scales much better over the number of consecutive calls, and
consistently has better performance than ours for sizes in the range $2^9$ to
$2^{13}$ w.r.t.\ ten additions and six multiplications. Comparing Futhark and
CUDA, we see that CUDA scales better in the number of arithmetic calls --
especially noticeable when going from one to ten additions.

In regard to multiplication: The Futhark implementation suffers from the
sub-optimal implementation described in \ref{subsec:mulfut}, but also from not
supporting 128-bit integer arithmetics. The CUDA implementation generally has
good performance, and in line with the expected outcome of the optimizations
(i.e.\ versions).

In regard to the addition: Both CUDA and Futhark have roughly the same
performance for one addition -- and it is a good performance, reaching 85\% of
peak device bandwidth. However, Futhark suffers a significant performance loss
when scaling to ten additions, while the CUDA versions sees a less significant
performance loss. The optimal versions in both languages have very consistently
performance across integer sizes and number of instances.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
