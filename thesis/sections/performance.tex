\section{Performance}
\label{sec:per}

To assess the efficiency of our implementations, we measure their relative
performance of running a benchmark suite. We expect that the CUDA
implementations yields the best performance given the underlying algorithms and
optimizations, and in turn, reveals the overhead and inefficiencies of the
Futhark implementations. We also run the benchmarks using the CGBN library
described in section \ref{sec:rel}. It enlights the relative performance of our
implementations against state of the art, in addition to the strong and weak
points of the underlying algorithms and overaching implementations strategy
(e.g.\ pros and cons of block-level versus warp-level processing of big
integers).

The runtimes of benchmarks can be tedious to directly compare. Instead, we
define some performance metrics based on the runtime, total number of bits, and
arithmetic operation, allowing us to directly compare results. We keep the total
number of bits fixed, but vary the size of integers and number of integer
instances across runs.

The structure is as follows: Section \ref{subsec:benchset} describes the setup
of our benchmarks, section \ref{subsec:perfmet} introduces the performance
metrics, and section \ref{subsec:benchres} reports our benchmark results.

\subsection{Benchmark Setup}
\label{subsec:benchset}

We have two benchmark types: The first type runs the arithmetic operators
straightforward, serving as the basis for evaluating performance. The second
type runs multiple consecutive applications of the operators, providing insight
on how well the arithmetics scales. The consecutive calls in CUDA are collected
in a single kernel to reduce memory overhead by keeping intermediate results in
shared memory. In Futhark we have consecutive intra-block calls, benchmarking
the compilers ability to fuse the arithmetics compared to CUDA. The consecutive
calls of the second type corresponds to computing $10(u+v)$ and $(u\cdot
v)^5$. Therefore, it is ten additions and six multiplications.

While we want to vary the size and number of big integers, the benchmark results
are only directly comparable if we keep the total amount of work fixed. E.g.\
consider a fixed work size of 256 bits; the benchmarks can be run with $n=1$
instance of base 32-bit big integers of size $m=8$, or, run with $n=1$, $m=4$,
and base 64-bit, or, run with $n=2$, $m=2$, and base 64-bit, etc.\ -- which all
uses the same number of bits in total. We choose the fixed total amount of work
to be $2^{32}$-bits, i.e.\ $2^{27}$ $\mathtt{u32}$-words or $2^{26}$
$\mathtt{u64}$-words. We measure accross the sizes of
$\{2^9,2^{10},\ldots,2^{18}\}$-bits big integers, and adjust the number of instances
accordingly.

Runtimes are averaged over multiple runs to improve stability and accuracy. In
CUDA we average across 200 runs for addition and 100 runs for multiplication,
with one dry-run before starting the timer. We transfer memory to the device
beforehand and stop the timer as soon as the kernel has completed its runs. In
Futhark we use the \texttt{futhark-bench} utility, that allows us to write
benchmark-specifications, similarly to \texttt{futhark-test}
\cite{futguide}. This utility also handles memory and do a dry-run before
starting the timer. It runs until the confidence level has reached 95\% (with at
least 10 runs) and reports both the average runtimes and the confidence
interval.

\subsection{Performance Metrics}
\label{subsec:perfmet}

Metrics are defined on an operator basis and derived from the complexities,
inner workings, and benchmark setup of the operators. Runtimes are measured in
microseconds.

\paragraph{Addition}
In Section \ref{subsec:addalg} we analyze the complexity of addition to work
$O(m)$ and depth $O(\log m)$. It is computationally efficient, and only require
communication w.r.t.\ the prefix sum. Hence, we expect addition to be
\textit{bandwidth} bound rather than \textit{operation} bound.

For $n$ instances of addition of big integers with size $m$, we calculate the
bandwidth (measured in GB/s) using the following formula:
\begin{equation}
    \label{eq:bandwidth}
    \mathit{bandwidth} = 3 \cdot \dfrac{n \cdot m \cdot (\mathtt{bits}/8)}{1000000000} \cdot \dfrac{1000000}{\mathit{runtime}}
\end{equation}
The first fraction represents how many gigabytes are in an array of big
integers, where the term $\mathtt{bits} / 8$ is the number of bytes in a word,
$n\cdot m$ is the total amount of words, and the denominator converts from bytes to
gigabytes. The second fraction is the runtime converted to seconds. The constant
3 is because addition must access global memory thrice -- twice for reading the
inputs and once for writing the output -- regardless of performing one or ten
consecutive additions, since intermediate results can remain in shared
memory. Thus, we use the same formula for both benchmark setups.


\paragraph{Multiplication}
In Section \ref{subsec:mulalg} we analyze the complexity of classical
multiplication to work $O(m^2)$ and depth $O(m)$. It is a computationally
demanding operation, and hence, we expect multiplication to be
\textit{operation} bound.

While we could center the metric around the number of \texttt{uint\_t}
operations, it leads to circuitous comparisons of benchmarks results across
bases. Instead, we normalize the results by choosing the number of
\texttt{uint32\_t} operations as the operational core for our metric. We then
define the metric as the number of gigaoperations per second (\textit{Gu32ops}).

The metric us not as clear-cut as the bandwidth, but we can estimate it. For
classical multiplication, the number of operations can be estimated by the
squared number of 32-bit words times the number of instances:
\begin{equation}
  \label{eq:u32opsobs}
  {n \cdot (m \cdot (\mathtt{bits} / 32))^2}
\end{equation}

However, classical multiplication is not asymptotically optimal. Instead, we
base our metric upon FFT multiplication, on the ground that it should reflect
the optimal scenario. We use the one proposed by Oancea and Watt in
\cite{oancea2024gpu}:
\begin{equation}
  \label{eq:u32ops}
  \mathit{Gu32ops} = \dfrac{300 \cdot n \cdot (m \cdot (\mathtt{bits} / 32)) \cdot \log (m \cdot (\mathtt{bits} / 32))}{1000000000} \cdot \dfrac{1000000}{\mathit{runtime}}
\end{equation}

When we compute six consecutive multiplications, we execute six times the number
of operations, and thus, we multiple the metric by 6 in these benchmarks.

\subsection{Benchmark Results}
\label{subsec:benchres}

The benchmarks are run on a NVIDIA GTX 1650 SUPER -- a GPU with 1280 CUDA cores,
4GB memory, 192.0 GB/s memory bandwidth, and 4.416 TFLOPS compute power
\cite{gpuspecs}.

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 62  & --   & 146 & 161 & --   & --   & --   & --   \\\hline
    $2^{17}$ & $2^{15}$ & 67  & --   & 161 & 163 & --   & --   & --   & --   \\\hline
    $2^{16}$ & $2^{16}$ & 19  & 168 & 166 & 166 & 116 & 154 & 158 & 156 \\\hline
    $2^{15}$ & $2^{17}$ & 19  & 168 & 167 & 166 & 145 & 168 & 168 & 146 \\\hline
    $2^{14}$ & $2^{18}$ & 84  & 168 & 168 & 166 & 150 & 168 & 168 & 168 \\\hline
    $2^{13}$ & $2^{19}$ & 164 & 168 & 168 & 164 & 156 & 168 & 168 & 168 \\\hline
    $2^{12}$ & $2^{20}$ & 165 & 168 & 168 & 166 & 156 & 168 & 142 & 168 \\\hline
    $2^{11}$ & $2^{21}$ & 164 & 169 & 163 & 166 & 134 & 165 & 75  & 168 \\\hline
    $2^{10}$ & $2^{22}$ & 157 & 108 & 93  & 166 & 71  & 91  & 38  & 168 \\\hline
    $2^{9}$  & $2^{23}$ & 119 & 55  & 47  & 167 & 36  & 47  & 20  & 168 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of one addition in base \texttt{u64} measured in GB/s (higher is better, 192 is peak)}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{|c|c||c?c|c|c?c|c|c|c|}\hline
    Bits & I{\footnotesize nstances} & CGBN & C-\texttt{V1} & C-\texttt{V2} & C-\texttt{V3}  & F-\texttt{V0} & F-\texttt{V1} & F-\texttt{V2} & F-\texttt{V3}\\\hline\hline
    $2^{18}$ & $2^{14}$ & 24  & --  & 92  & 83  & --  & --  & --  & --  \\\hline
    $2^{17}$ & $2^{15}$ & 57  & --  & 111 & 94  & --  & --  & --  & --  \\\hline
    $2^{16}$ & $2^{16}$ & 73  & 41 & 123 & 107 & --  & --  & --  & --  \\\hline
    $2^{15}$ & $2^{17}$ & 45  & 48 & 128 & 107 & 15 & 24 & 15 & 13 \\\hline
    $2^{14}$ & $2^{18}$ & 98  & 51 & 112 & 107 & 18 & 27 & 16 & 13 \\\hline
    $2^{13}$ & $2^{19}$ & 162 & 46 & 83  & 108 & 18 & 30 & 14 & 13 \\\hline
    $2^{12}$ & $2^{20}$ & 164 & 35 & 44  & 108 & 16 & 29 & 14 & 13 \\\hline
    $2^{11}$ & $2^{21}$ & 162 & 24 & 23  & 108 & 15 & 25 & 10 & 13 \\\hline
    $2^{10}$ & $2^{22}$ & 153 & 12 & 12  & 108 & 7  & 13 & 5  & 13 \\\hline
    $2^{9}$  & $2^{23}$ & 110 & 6  & 6   & 108 & 4  & 7  & 3  & 13 \\\hline
  \end{tabular}
  \caption{\footnotesize Performance of ten additions in base \texttt{u64} measured in GB/s (higher is better, 192 is peak)}
\end{table}


{\red [Missing]}

\subsection{Discussion}
{\red [Missing]}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
