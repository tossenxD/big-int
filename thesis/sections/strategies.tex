\section{Implementation and Optimization Strategy}
\label{sec:strat}

For the arithmetics to be efficient, they must be implemented at a CUDA
block-level, thus, minimizing the communication overhead by utilizing the faster
block-level shared memory for intermediate results. For CUDA, this is achieved
manually by the execution configuration and the methods deployed to index over
kernel parameters. However, for Futhark, the compiler ultimately determines how
to map it. Hence, we use the compiler attribute \texttt{\#[only\_intra]} when
batch processing arithmetics in Futhark, telling the compiler to only map the
arithmetics at block-level (intra-block).

In principle, any arithmetic function only requires global memory access to read
the inputs and write the output. In Futhark, this can be a bit involved,
discussed further in section \ref{subsec:addfut}. In CUDA however, we always
assume the following kernel structure: Fetch inputs from global memory
coalesced; execute the function body; write results to global memory
coalesced. From now on, we refer to the function body when discussing kernels.

Given we optimize for block-level, the arithmetics are confined to run on at
most 1024 threads.  Hence, the implementation is aimed at medium-sized big
integers, as when the block-size exceeds 1024, the arithmetics is not able to
run. This is an artificial barrier, as we can introduce sequentialization within
each thread to allow bigger integer sizes. E.g. consider the example given in
Figure \ref{fig:seqexa}. In the left illustration, each of the $512$ digits of
the big integer is handled by a separate thread, giving $512$ threads. On the
right, each thread instead handle two digits sequentially, giving only $256$
threads.

\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ & $t_1$ & $t_2$ & $\cdots$ & $t_{510}$ & $t_{511}$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $\shortdownarrow$ & $\shortdownarrow$ & $\shortdownarrow$ & & $\shortdownarrow$ & $\shortdownarrow$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
      \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ &  & $t_1$ & $\cdots$ & $t_{255}$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}}
        $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \caption{\footnotesize Example of sequentialization within parallel threads.}
    \label{fig:seqexa}
\end{figure}

However, when we increase the sequentialization factor within a thread, we
decrease the amount of parallelism within the program. Thus, the bigger the
integer, the less efficient the implementation (for sizes that otherwise would
exceed CUDA block limits). This only holds up to a certain input size, because
the block-level shared memory also grows with the integer size, resulting in an
out-of-memory error for integers too big.

Likewise, we can also have big integer sizes that are too small to efficiently
compute in parallel. Consider again the example of Figure \ref{fig:seqexa} and
suppose array $w$ consists of 16 digits instead of 512 -- then we have 16 and 8
threads to handle $w$, respectively, neither of which fits in a warp. I.e.\ we
have 16 and 24 threads, respectively, that idles when computing $w$.

This is not a problem when we only compute arithmetics on a single big integer,
but can significantly impede performance when batch processing. Suppose we
process 1024 big integers of size 16, meaning 1024 CUDA blocks with one warp per
block. Furthermore, suppose we only have access to 512 warps on our GPU. Then,
each warp must compute two blocks, and within each block, half of threads are
idle -- in principle giving us half the throughput compared to 1024 integers of
size 32.

Again, this is an artificial barrier, as we can conjoin two big integers and
handle them segmented within a warp. This process is called \textit{flattening},
and operations working on flattened data structures are called \textit{segmented
  operations}. Thus, in this example, we now get 512 blocks, which can be
computed by the 512 warps in one sweep. This is not necessarily double the
performance, since flattening, unflattening, and segmented operations comes with
a cost, but in these cases, the cost may be well worth it when handling many
arithmetic instances of smaller big integers.

% if literature on flattening is needed, maybe this?
% \cite{Henriksen:2017:FPF:3062341.3062354}

In regards to the strategy of implementing these optimizations, we take
inspiration from the GMP library: They write multiple parameterized versions of
each arithmetic function (including multiple algorithms), and dynamically (or by
tuning) choose the best performing based on the current size \cite{GMP}.

Inspired by their strategy, we implement multiple versions of arithmetics, with
a varying degree of optimizations, and determine what version (optimizations)
outperforms the others, by an experimental approach over the integer size and
number of arithmetic instances. Since performance is the main concern, we assume
that the size and number of inputs divides exact to implementations exhibiting
sequentialization and segmentation.\footnote{This assumption does not restrict
  the usefulness of the implementations, as we could simply pad inputs.}



%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
