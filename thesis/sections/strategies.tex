\section{Implementation and Optimization Strategy}
\label{sec:strat}

For the arithmetics to be efficient, they must be implemented at a CUDA
block-level. I.e., the arithmetic operations are confined to run on at most 1024
threads. This reduces the communication overhead by utilizing the faster
block-level shared memory for intermediate results, meaning that, in principle,
any arithmetic program only have access global memory when reading the input big
integer(s) and writing the output big integer.

Hence, the implementation is more so aimed at medium-sized big integers, as when
the size exceeds 1024, the arithmetics is not able to run. This is an artificial
barrier, as we can simply introduce sequentialization within each thread to up
the size. E.g. consider the example given in Figure \ref{fig:seqexa}. In the
left illustration, each of the $512$ digits of the big integer is handled by a
separate thread, giving us $512$ threads. On the right, each thread instead
handle two digits by doing some sequential computations, giving us only $256$
threads.

\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ & $t_1$ & $t_2$ & $\cdots$ & $t_{510}$ & $t_{511}$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $\shortdownarrow$ & $\shortdownarrow$ & $\shortdownarrow$ & & $\shortdownarrow$ & $\shortdownarrow$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
      \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ &  & $t_1$ & $\cdots$ & $t_{255}$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}}
        $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \caption{Example of sequentialization within workthreads.}
    \label{fig:seqexa}
\end{figure}

However, when we increase the sequentialization factor within a thread, we
decrease the amount of parallelism within the program. Thus, bigger the integer the
less efficient the implementations becomes (for sizes that would exceed CUDA
block limits).

Likewise, we can also have big integer sizes that are too small to efficiently
compute in parallel. Consider again the example of Figure \ref{fig:seqexa} and
suppose $w$ only consists of 16 digits instead of 512 - then we have at most 16
threads to handle $w$. A CUDA warp is 32 threads, meaning, that we have 16
threads that remain idle throughout the computation of $w$.

This is not a problem when we only compute arithmetics on a single big integer,
but suppose we have 1024 big integers instead. This means that we launch 1024
CUDA blocks - one for each digit - consisting of one warp each. Suppose we only
have access to 512 cores. Then, each core must compute two blocks, and within
each block, half of threads are idle.

Again, this is an artificial barrier, as we can conjoin two big integers and
handle them segmented by 32 threads. Thus, in this example, we now get 512
blocks, which can be computed by the 512 cores in one sweep. This is not
necessarily double the performance, as conjoining, splitting and segmented
operations comes with a cost, but, in most cases, the cost is constant, and can
be neglected when handling many arithmetic instances.

The GMP library is built on a smart performance optimization strategy: They
write multiple parameterized versions of each arithmetic function (including
multiple algorithms), and dynamically (or by tuning) choose the best performing
based on the current size \cite{GMP}.

Inspired by their strategy, we will implement multiple \textit{versions} of the
algorithms presented in this Thesis. We determine what version outperformes the
others in an experimental approach over the integer size and number of
arithmetic instances, and will incorporate the results into the produced
Futhark library.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
