\section{Overview of Implementation Strategy}
\label{sec:strat}

For the arithmetics to be efficient, they must be implemented at a CUDA
block-level, thus, minimizing the communication overhead by utilizing the faster
block-level shared memory for intermediate results. In CUDA programs, this is a
manual process achieved by the execution configuration and the methods deployed
to index over kernel parameters. However, for Futhark, the compiler ultimately
determines how to map it. Hence, we use the compiler attribute
\texttt{\#[only\_intra]} when batch processing arithmetics in Futhark, telling
the compiler to only map the arithmetics at block-level (intra-block).

In principle, any arithmetic function only requires global memory access to read
the inputs and write the output. In Futhark, this can be a bit involved,
discussed further in section \ref{subsec:addfut}. In CUDA however, we always
assume the following kernel structure: Fetch inputs from global memory
coalesced; execute the function body; write results to global memory
coalesced. From now on, we refer to the function body when discussing kernels.

Given we optimize for block-level, the arithmetics are confined to run on at
most 1024 threads.  Hence, the implementation is aimed at medium-sized big
integers, as when the block-size exceeds 1024, the arithmetics are not able to
run. This is an artificial barrier, as we can introduce sequentialization within
each thread to allow bigger integer sizes. E.g.\ consider the example given in
Figure \ref{fig:seqexa}. In the left illustration, each of the $512$ digits of
the big integer is handled by a separate thread, giving a block size of
$512$. In the right illustration, each thread handles two digits sequentially,
giving a block size of $256$ instead.

\begin{figure}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ & $t_1$ & $t_2$ & $\cdots$ & $t_{510}$ & $t_{511}$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $\shortdownarrow$ & $\shortdownarrow$ & $\shortdownarrow$ & & $\shortdownarrow$ & $\shortdownarrow$\\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
      \centering
      \begin{tabular}{C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.6cm}}
        $t_0$ &  & $t_1$ & $\cdots$ & $t_{255}$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}C{0.105cm}}
        $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ &  &  & $\shortdownarrow$ & $\diagonalarrowdown$ & \\
      \end{tabular}\\[-0.5ex]
      \begin{tabular}{|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|C{0.6cm}|}
        \hline
        $w_0$ & $w_1$ & $w_2$ & $\cdots$ & $w_{510}$ & $w_{511}$\\
        \hline
      \end{tabular}\\
    \end{minipage}
    \caption{\footnotesize Example of sequentialization within parallel threads to reduce CUDA block size.}
    \label{fig:seqexa}
\end{figure}

However, when the sequentialization factor within parallel threads increases,
the amount of parallelism within the program decreases. Furthermore,
sequentially handling many digits per thread may exhaust the local memory. Thus,
the bigger the integers are, the less efficient the implementation becomes (for
sizes that otherwise would exceed CUDA block limits). This only holds up to a
certain input size, because the block-level shared memory also grows with the
integer size, resulting in an out-of-memory error for integers too big.

Likewise, we can also have big integer sizes that are too small to efficiently
compute in parallel. Suppose that each thread process a digit of the input, and
the input consists of 16 digits -- then we have 16 threads processing and 16
threads idling. This is not a problem when we only compute arithmetics on a
single big integer, but effectively halves throughput when batch processing
enough integers to consume the whole device.

Again, this is an artificial barrier, as we can combine (\textit{flatten}) big
integers and handle them \textit{segmented} within a block. E.g.\ combining two
integers per block in the example above is enough to fulfill a warp. This is not
necessarily double the performance, since flattening, unflattening, and
segmented operations comes with a cost, but the cost is overshadowed by the
benefits (no idling threads) when processing many instances over smaller
integers. \bigskip

To summarize, the arithmetics are implemented at CUDA block-level aimed at
medium-sized integers. They must increase the sequentialization factor within
threads to allow bigger input integers (up to a limit). The throughput may
improve from integrating segmented operations, processing multiple instances of
integers per CUDA block.

We take inspiration from the strategy behind the GMP library: They write
multiple parameterized versions of each arithmetic function (including multiple
algorithms), and dynamically (or by tuning) choose the one performing best based
on the input size \cite{GMP}. While we do not implement multiple algorithms, we
implement multiple versions of the same algorithm with a gradual degree of
optimization -- e.g.\ with and without the ability to process multiple instances
per block.

We expect the most optimized version to consistently perform best, but run
experiments and benchmarks over the size of integers on all versions. It reveals
the performance gain of each optimization and possibly find versions most suited
for specific inputs. Since performance is the main concern, we assume that the
size and number of integers per block exactly divides the block dimensions for
kernels exhibiting sequentialization and segmentation.\footnote{Assumption does
  not restrict the usefulness of the implementations since inputs can be padded
  to fit.}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
